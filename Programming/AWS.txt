--> Cloud Computing
Cloud computing is the on-demand delivery of compute power, database, storage, application, and other IT resources. It allows users to access these resources instantly, without the need for advance ordering or building their own data centers. Here are some key points about cloud computing:
On-demand delivery: Cloud computing provides resources when they are needed, allowing users to scale up or down as required.
Pay-as-you-go pricing: Users only pay for the resources they use, when they use them. There is no need for upfront investment or long-term commitments.
Provisioning: Cloud service platforms allow users to provision the exact type and size of computing resources they need. Whether it's a big server or a small one, the cloud can accommodate the user's requirements.
Instant access: Resources can be accessed instantly, without the need for 24 or 2 hours notice. Users can have access to servers, storage, databases, and application services within seconds.
Interface: Cloud computing provides a user-friendly interface that allows easy access to servers, storage, databases, and application services. This interface makes it convenient for users to manage and utilize their resources.

Types of Cloud Computing -
1. Infrastructure as a Service (IaaS)
Provides the fundamental building blocks for cloud computing.
Offers networking, computers, and data storage space in their raw form.
Provides a high level of flexibility and allows easy migration from traditional on-premises IT to the cloud.
Examples of IaaS services include Amazon EC2, Google Cloud, Azure, Rackspace, and Digitalocean.

2. Platform as a Service (PaaS)
Removes the need for organizations to manage the underlying infrastructure.
Focuses on the deployment and management of applications.
Allows developers to concentrate on developing and running their applications without worrying about infrastructure management.
AWS manages everything from the runtime to the networking.
Examples of PaaS services include AWS Elastic Beanstalk, Google App Engine, and Microsoft Azure App Service.

3. Software as a Service (SaaS)
Provides a complete product that is run and managed by the service provider.
Users can access the software through the internet without the need for installation or maintenance.
Eliminates the need for organizations to manage software updates, security, and infrastructure.
Examples of SaaS services include Salesforce, Google Workspace, Microsoft Office 365, and Dropbox.

Types of Clouds -
1. Private Cloud
Used by a single organization and not exposed to the public.
Examples: Rackspace
Provides a private data center managed by a third party.
Offers complete control and more security for sensitive applications.

2. Public Cloud
Owned and operated by a third-party cloud service provider.
Resources are delivered over the internet.
Examples: Microsoft Azure, Google Cloud, Amazon Web Services (AWS).

3. Hybrid Cloud
Hybrid cloud combines private and public cloud infrastructure.
Some servers are kept on-premises while certain capabilities are extended to the cloud.
Provides control over sensitive assets in the private infrastructure.
Offers flexibility and cost-effectiveness of the public cloud.

--> IAM(Identity & Access Management)
It is a Global service in which we are going to create our users and assign them to groups. Groups only contains users, not other groups. Users don't have to belong to a group, and user can belong to multiple groups. We create users or groups because we want to allow them to use our AWS accounts and we have to give them permissions. We add inline policies to those users who are not in any group. We protect users and groups by defining the password policy. The second option is using Multi Factor Authentication(MFA) which is password + security device you own to login the account. MFA devices options are - Virtual MFA device, Universal 2nd Factor Security Key etc.

Permissions -
Users or Groups can be assigned JSON documents called policies. These polices define the permissions of the users. In AWS you apply the least privilege principle that is don't give more permissions than a user needs.

IAM Policies Structure -
Version: policy language version
Id: an identifier for the policy (optional)
Statement: one or more individual statements (required)
Sid: an identifier for the statement (optional)
Effect: whether the statement allows or denies access
Principal: account/user/role to which this policy applied to
Action: list of actions or set of API calls this policy allows or denies based on the Effect
Resource: list of resources to which the actions applied to
Condition: conditions for when this policy is in effect (optional)

IAM Roles for Services -
Some AWS services will need to perform actions on your behalf. To do so, we'll assign permissions to AWS services with IAM Roles. Roles allows entities in the AWS to get credentials for a short duration and to do whatever they need to.

--> EC2
Elastic Compute Cloud is a service provided by AWS that allows users to rent virtual servers in the cloud, known as EC2 instances, and use them as compute resources. EC2 instances are bound to an Availibility Zones(AZ).
EC2 is composed of several components -
EC2 instances: Virtual machines that can be rented from AWS.
EBS volumes: Virtual drives used for storing data.
Elastic Load Balancer: Distributes load across multiple EC2 instances.
Auto Scaling Groups (ASG): Allows for scaling of services based on demand.

Choosing EC2 Instances -
When creating an EC2 instance, users have several options to choose from:
Operating System: Linux, Windows, or Mac OS.
Compute Power: Users can select the desired CPU and number of cores.
Random Access Memory (RAM): Users can choose the amount of RAM required.
Storage Space: Users can choose between network-attached storage (EBS or EFS) or hardware-attached storage (EC2 instance store).
Network Configuration: Users can select the type of network card and public IP.
Security Group: Users can define firewall rules for their EC2 instance.
Bootstrap Script (EC2 User Data): A script that configures the instance during its initial launch.

Instance Types -
Instances are categorized into families based on their intended use cases and performance characteristics.
Choosing the right instance type is important to ensure optimal performance for your application.
Some common instance families include:
General Purpose (e.g., t2, m5)
Compute Optimized (e.g., c5, r5)
Memory Optimized (e.g., x1, z1d)
Storage Optimized (e.g., i3, d2)
Naming convention:
m5.2xlarge (m is instance class, 5 is generation, 2xlarge is the size within the instance class)

EC2 User Data and Bootstrapping -
Bootstrapping refers to the process of launching commands or scripts when a machine starts.
The EC2 User Data script is run only once, during the initial boot of the instance.
The purpose of the EC2 User Data script is to automate boot tasks, hence the name bootstrapping.
The tasks you can automate using the User Data script can include installing updates, software, or downloading files from the internet.
The more tasks you add to your User Data script, the longer it will take for your instance to boot.
The EC2 User Data script runs with root user privileges, so any commands you include will have sudo rights.

Security Groups -
Security groups are a fundamental aspect of network security in the AWS cloud.
Security groups act as firewalls around EC2 instances that determine what traffic is allowed to enter and exit the EC2 instances.
Security groups can have rules that reference either IP addresses or other security groups.Rules define whether inbound traffic from the outside to the EC2 instance is allowed, and if the EC2 instance can perform outbound traffic to the internet.
Security groups can be attached to multiple EC2 instances. There is not a one-to-one relationship between security groups and instances.
Similarly, an EC2 instance can have multiple security groups attached to it.
Instances are associated with security groups, and security groups are associated with rules.

Impact of Region and VPC Changes -
Security groups are tied to a specific region and VPC combination.
If you switch to another region or create a new VPC, you need to create new security groups.
This means that security groups are not portable across regions or VPCs.
It is important to consider this when planning and managing your security groups.

Troubleshooting Security Group Issues -
If you are unable to connect to an EC2 instance on a specific port and your computer hangs without any response, it is likely a security group issue. This is because the security group is blocking the inbound traffic to that port. On the other hand, if you receive a "connection refused" error, it means that the security group allowed the traffic to reach the instance, but the application running on the instance is not responding or not launched properly.
By default, all inbound traffic is blocked and all outbound traffic is authorized.
This means that you can initiate outbound connections from your instances, but incoming connections are not allowed unless explicitly configured in the security group rules.

Referencing Security Groups from Other Security Groups -
AWS provides a feature that allows you to reference one security group from another security group. This is particularly useful when working with load balancers. By referencing a security group in the inbound rules of another security group, you can allow instances associated with the second security group to communicate with instances associated with the first security group. This eliminates the need to specify IP addresses and simplifies the configuration process.
For example, if you have an EC2 instance with security group "Group 1" and you authorized inbound traffic from "Group 2" then any instance associated with "Group 2" can communicate with the instance associated with "Group 1" on the specified port.

Importance of SSH Access -
SSH access is often the most critical and sensitive aspect of security. 
It is recommended to maintain a separate security group specifically for SSH access.
This helps ensure that SSH access is properly configured and secured.
By dedicating a separate security group, it becomes easier to manage and monitor SSH traffic.
SSH(Secure Shell) allows you to control a remote machine or server using CLI.
Linux and windows command - ssh -i '.\EC2 Tutorial.pem' ec2-user@65.2.57.62 (.pem is ssh key, 65.... is public IP address)

--> EC2 Instance Storage
EBS volumes -
Elastic Block Store (EBS) is a block-level storage service provided by AWS. It allows you to create and attach persistent block storage volumes to your EC2 instances. They allow data to persist even after the instance is terminated.
Each EBS volume can only be mounted to one instance at a time.
When creating EBS volumes, you need to specify the AZ in which the volume will be created & it is bound to that specific availability zone.
EBS volumes are network drives, not physical drives and are attached to instances through the network..
Communication between the instance and the EBS volume is done through the network, which may introduce some latency.
EBS volumes can be quickly detached from one EC2 instance and attached to another.
EBS volumes are locked to specific availability zones, meaning they can only be attached to instances in the same availability zone.
When creating EBS volumes through EC2 instances, there is an attribute called "delete on termination". This attribute determines whether the EBS volume should be deleted automatically when the associated EC2 instance is terminated.
By default:
The "Delete on Termination" option is ticked for the root volume.
The "Delete on Termination" option is not ticked for any other attached EBS volumes.

EBS Snapshots -
It makes a backup of your EBS volume at a point of time. It is not necessary to detach volume to do snapshot. Can copy snapshots across AZ or regions, so that you would be able to transfer some of your data in a different region.

AMI -
AMI stands for Amazon Machine Image. AMIs represent a customization of an EC2 instance.
AMIs can be created by AWS or customized by users. AMIs contain software configurations, including the operating system and monitoring tools.
Behind the scenes, when we create an AMI, EBS snapshots are also created.
These snapshots capture the data stored on the instance's EBS volumes.
They provide a backup of the instance's data and can be used to restore or create new volumes.
Benefits of Using AMI:
Faster boot time and configuration time.
Prepackaged software installation on EC2 instances.
Can be built for a specific region and copied across regions. Allows for leveraging the AWS global infrastructure.

Types of AMIs -
1. Public AMIs
Provided by AWS. Can be used by all AWS users. No need to create or maintain them.
Examples: Amazon Linux 2 AMI.

2. Custom AMIs
Created and maintained by users. Can be built for specific requirements. Automation tools available for AMI creation. Users are responsible for creating and maintaining them.

3. AWS Marketplace AMIs
Created by vendors and potentially sold by them. Can save time by using pre-configured software. Users can also create and sell their own AMIs on the AWS Marketplace.

AMI Creation Process -
AMIs are a powerful tool for creating customized EC2 instances with pre-configured software and settings.
Launching an Instance in us-east-1a:
We will start by launching an instance in the us-east-1a region.
This instance will serve as the basis for our custom AMI.

Creating a Custom AMI:
Once we have customized the instance, we will create an AMI from it.
This AMI will capture the configuration and state of the instance at the time of creation.
It will serve as a template for launching new instances with the same configuration.

Launching Instances from the Custom AMI in us-east-1b:
In the us-east-1b region, we will be able to launch instances from the custom AMI we created.
This will effectively create a copy of our EC2 instance in a different region. We can launch multiple instances from the same AMI if needed.

EC2 Image Builder -
EC2 Image Builder is a service provided by AWS that allows you to automate the creation, maintenance, validation, and testing of Amazon Machine Images (AMIs) for EC2 instances. This service simplifies the process of building and customizing software components on EC2 instances, ensuring that your AMIs are up-to-date and meet your specific requirements.

EC2 Image Builder Working -
Setup: You configure and set up the EC2 Image Builder service, specifying the desired components and customizations for your AMIs.
Builder EC2 Instance: When the EC2 Image Builder service runs, it automatically creates a builder EC2 instance. This instance is responsible for building components and customizing the software according to your specifications. For example, it can install Java, update the CLI, update the software system, install firewalls, or even install your application.
AMI Creation: Once the builder EC2 instance completes its tasks, EC2 Image Builder creates an AMI from that instance. This AMI represents the desired state of your EC2 instances, including all the customizations and software installations.
Validation: To ensure the quality and functionality of the AMI, EC2 Image Builder automatically creates a test EC2 instance from the newly created AMI. You can define a set of tests to be run on this instance, such as checking if the AMI is working correctly, if it is secure, or if your application is running properly. You have the flexibility to skip certain tests if needed.
Distribution: Once the AMI passes the validation tests, it is ready for distribution. Although EC2 Image Builder is a regional service, you can distribute the AMI to multiple regions, allowing your application and workflow to be globally available.

EC2 Instance Store -
The EC2 Instance Store is a type of hardware disk that can be attached directly to an EC2 instance. It provides extremely high disk performance and is a great choice when you need better I/O performance and good throughput.
The storage provided by the EC2 Instance Store is considered ephemeral, meaning that if you stop or terminate the EC2 instance, the data stored on the instance store will be lost. Therefore, it is not suitable for long-term storage.
The EC2 Instance Store is ideal for use cases where you need temporary storage or scratch data. It can be used as a buffer or cache for storing data that is not critical or does not need to be persisted long-term.

EFS -
EFS stands for Elastic File System and is a managed network file system.
It allows you to attach a network file system to multiple EC2 instances simultaneously.
Unlike EBS volumes, which can only be attached to one instance at a time, EFS can be mounted onto hundreds of instances.
EFS is a shared network file system, also known as a shared NFS (Network File System).
It is designed to work with Linux EC2 instances and this architecture allows for easy sharing and access to files across instances in different availability zones.

--> Elastic Load Balancing & ASG
Scalability and High Availability -
Scalability allows an application to handle greater loads by adapting.
There are two types of scalability in the cloud:
1. Vertical scalability
Increasing the size or capacity of an instance. Example: Changing from t2.micro to t2.large.
Vertical scalability is common in non-distributed systems like databases.

2. Horizontal scalability (elasticity)
Increasing the number of instances. Improves performance and capacity. Increase fault tolerance and availability.
In AWS, achieving horizontal scalability:
Using Elastic Load Balancing (ELB) to distribute traffic across multiple instances.
Auto Scaling Groups (ASG) to automatically add or remove instances based on demand.
Horizontal scalability is commonly used in distributed systems to handle large workloads.

Scalability is linked but different from high availability.
High availability refers to the ability of an application to remain operational and accessible even in the face of failures or disruptions. It is recommended to use at least two availability zones for high availability.
Achieving high availability in AWS:
Using Elastic Load Balancing (ELB) to distribute traffic across multiple instances.
Auto Scaling Groups (ASG) to automatically replace failed instances.
Deploying resources across multiple Availability Zones (AZs) to ensure redundancy.

Scalability vs Elasticity vs Agility
Scalability: Ability for a system to accommodate a larger load by making the hardware stronger (scaling up) or adding nodes (scaling out).
Elasticity: System's ability to automatically scale based on the load it receives.
Agility: New IT resources are only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes.

Elastic Load Balancing -
Elastic Load Balancing (ELB) is a service provided by AWS that allows for better scalability and availability of backend EC2 instances and they can be spread out across multiple AZ. It acts as a server that forwards internet traffic to multiple downstream servers, which are the backend EC2 instances. Load balancers handle upgrades, maintenance, and high availability, while users only need to configure their behavior.
Types of Load Balancers:
1. Application Load Balancer (ALB)
Layer 7 load balancer for HTTP and HTTPS traffic.
Supports HTTP routing features and static DNS for a static URL.
Simple architecture: Users access the load balancer using HTTP or HTTPS protocols.

2. Network Load Balancer (NLB)
Ultra-high performance load balancer.
Layer 4 load balancer for TCP and UDP traffic.
Suitable for applications that require low latency and high throughput.
Simple architecture: Users access the load balancer using TCP or UDP protocols.

3. Gateway Load Balancer (GWLB)
Layer 3 load balancer.
It is designed to route traffic to third-party security virtual appliances, such as firewalls, intrusion detection systems, or deep packet inspection systems.
GWLB allows you to inspect IP packets and perform firewall operations before forwarding the traffic to your applications.
It supports the Geneve protocol for routing traffic to virtual appliances running on EC2 instances.
GWLB architecture involves sending traffic to EC2 instances running virtual appliances, analyzing the traffic, and then forwarding it back to the load balancer for further routing to the applications.

Auto Scaling Groups -
An autoscaling group (ASG) is a feature provided by cloud service providers, that allows for the automatic creation and termination of instances based on the current demand or load on an application. ASGs are commonly used in conjunction with load balancers to ensure efficient and scalable application deployments. We can spread our load across multiple AZ.
The goal of ASG is to:
Scale out (add EC2 instances) to match an increased load.
Scale in (remove EC2 instances) to match a decreased load.
Ensure we have a min and max number of machines running.
Replace unhealthy instances.

ASG in AWS with Load Balancer -
When using an autoscaling group with EC2 instances, the load balancer plays a crucial role in distributing traffic.
Initially, if there is only one EC2 instance in the autoscaling group, the load balancer will redirect all incoming web traffic to that instance.
As the autoscaling group scales out by adding more EC2 instances, the load balancer will register them and distribute traffic across all instances.
The load balancer dynamically adjusts the distribution of traffic based on the number of instances available.
This ensures that the workload is evenly distributed and can handle increased traffic.
The load balancer can distribute traffic to the maximum size of the autoscaling group if it scales to that point.

--> S3
Amazon S3 stands for Simple Storage Service and is a cloud storage service provided by Amazon Web Services (AWS).
It allows you to store and retrieve large amounts of data, such as files, documents, images, videos, etc. It offers a wide range of use cases, including backup and storage, disaster recovery, archival purposes, hybrid cloud storage, hosting applications and media, data lakes, big data analytics, delivering software updates, and hosting static websites.
Files in Amazon S3 are called objects.
Files in Amazon S3 are stored in buckets, which can be seen as top-level directories.
Buckets are created in your account and must have a globally unique name.
S3 looks like a global service but buckets are created in a region.
S3 pre-signed URL contains a signature that verifies the credentials in the encoded URL to allow the user to show its object. 
S3 Objects:
Objects in S3 are files.
Each object has a key, which is the full path of the file.
The key of a file in the top-level directory is the same as the object name.
If a file is nested in folders, the key is the full path of the file. The key is composed of a prefix (folder path) and an object name.
S3 does not have a concept of directories, everything is a key.
Objects in Amazon S3 can have metadata associated with them. Metadata consists of a list of key-value pairs that provide additional information about the object. Metadata can be set by the system or by the user.
Each object can have Tags (up to 10 Unicode key-value pairs as metadata which are useful for security/lifecycle).
Object can have Version ID (if versioning is enabled).
Object Values:
The values of S3 objects are the content of the file.
You can upload any type of file to S3. There is a maximum size limit for objects in S3.

S3 Security -
1. User-Based Security
IAM policies determine which API calls are allowed for a user. IAM principles can access S3 objects if:
IAM permissions allow it.
Resource policies allow it.
There is no explicit deny in the action.

2. Resource-Based Security
(i) Bucket Policies
S3 bucket policies are bucket-wide rules that can be assigned from the S3 console.
Bucket policies allow specific users or users from other accounts (cross-account) to access the S3 bucket.
Bucket policies can be used to make S3 buckets public.
(ii) Object Access Control List (ACL)
ACL provides finer-grained security for S3 objects. ACL can be disabled.
(iii) Bucket-level ACL is less common but can also be disabled.

3. Object Encryption
Objects in S3 can be encrypted using encryption keys.

Bucket Policies -
Bucket policies are the most common way to secure an S3 bucket. Bucket policies are JSON-based policies.
The structure of a bucket policy:
Resource: Specifies the buckets and objects the policy applies to.

Granting Public Access
To grant public access to a bucket or objects within it, we can use an S3 bucket policy.
For example, we can set a bucket policy that allows anyone (*) to perform the GetObject action on all objects within the bucket.
This would make all objects within the bucket publicly readable.

Granting Access to IAM Users
If we have an IAM user within our AWS account who needs access to the S3 bucket, we can assign IAM permissions to that user through a policy.
By creating a policy that allows access to the S3 bucket, the user will be able to access the bucket.

Granting Access from EC2 Instances
If we want to give access from an EC2 instance to an S3 bucket, IAM users are not appropriate. Instead, we need to use IAM roles.
We can create an IAM role for the EC2 instance with the necessary IAM permissions to access the S3 bucket.
By assigning this role to the EC2 instance, it will be able to access the S3 bucket.

Allowing Cross-Account Access
If we want to allow access from an IAM user in another AWS account, we can use a bucket policy.
We can create an S3 bucket policy that allows cross-account access.
By specifying the IAM user in the other AWS account in the bucket policy, we can grant them access to the S3 bucket.

Static Website Hosting, Versioning, Replication, S3 Storage Classes -
Easy topics, if you forgot these watch a quick tutorial.

AWS Snow Family -
The AWS Snow Family is a collection of highly secure portable devices that serve two main use cases within AWS: data migration and edge computing. AWS OpsHub is a software to connect & manage your Snow Family Devices.
Data migration refers to the process of transferring large amounts of data in and out of AWS. The AWS Snow Family offers three different types of devices for data migration: Snow Cone, Snowball Edge, and Snowmobile and each option has different storage capacities 8 terabytes, 80 terabytes, and 100 petabytes respectively.
Recommended migration size for Snow Cone is up to 24 terabytes online and offline and DataSync agent is pre-installed on this.
Snowball Edge is suitable for petabyte migrations and requires offline data transfer & supports storage clustering to put 15 of them together.
Snowmobile is the best choice for transferring more than ten petabytes of data.

Edge computing is a concept where data processing and analysis is performed closer to the source of data creation, rather than sending it back to the cloud. AWS Snow Family provides a range of devices that enable edge computing capabilities which are Snowcone, Snowcone SSD, Snowball Edge and they can run EC2 instances & AWS Lambda functions on them.

Storage Gateway -
It is a hybrid solution to extend on-premises storage to S3. Bridge b/w data stored on your end & cloud data in S3.
Summarizing the storage options on AWS:
Block storage will be EBS, EC2 instance store.
File storage will be EFS.
Object storage will be S3, Glacier.

--> Database Analytics
RDS -
RDS stands for Relational Database Service. It is a managed database service provided by AWS for relational databases.
RDS uses SQL as the query language. It allows you to create and manage databases in the cloud.
Supported databases include PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, and Aurora (proprietary database from AWS).

Aurora -
Aurora is a database technology created by AWS. It is not open source and is designed to be cloud optimized.
Aurora is not included in the free tier of AWS. Aurora supports two types of databases: PostgreSQL and MySQL.
Aurora offers a 5x performance improvement over MySQL on RDS and a 3x performance improvement over PostgreSQL on RDS.
Storage for Aurora databases automatically grows in increments of 10GB up to 128 terabytes.
Aurora is more expensive than RDS (about 20% more), but it is more efficient and cost-effective.

RDS Database Deployment Options -
RDS Read Replicas -
Read replicas are used to scale the read workload of an RDS database.
By creating read replicas, copies of the main RDS database are created, allowing applications to read from these replicas.
This distributes the read workload across multiple RDS databases. Up to 15 read replicas can be created for an RDS database.
Writing data is still done only to the main database.
Multi-AZ Deployment -
Multi-AZ deployment is used to achieve high availability in case of an availability zone (AZ) outage.
A replication is set up across a different AZ, creating a failover database.
If the main RDS database crashes or the AZ experiences issues, RDS triggers a failover.
Data is only read and written to the main database.
The failover database remains passive and is not used for active read or write operations until there is a failure in the main DB.
Multi-Region Deployment -
In a multi-region deployment, read replicas of the RDS database are created in different regions.
Read replicas allow applications in different regions to read data locally, reducing latency.
However, writes still need to happen cross-region, which may introduce some latency.
This deployment is useful for setting up a disaster recovery strategy in case of a regional issue.
It provides better performance for applications in different regions.

Amazon Elasticache -
Amazon Elasticache is a type of database service provided by AWS. It is used to manage in-memory databases, specifically Redis or Memcached. These in-memory databases offer high performance and low latency.
The architecture for using Elasticache involves an elastic load balancer directing traffic to EC2 instances. These instances read and write data from the main Amazon RDS database. If possible, they also cache certain values in the Elasticache database. This helps in offloading the pressure from the main database and improving overall performance.

DynamoDB -
DynamoDB is a fully managed, highly available database provided by AWS. It is part of the NoSQL database family, meaning it is not a relational database. DynamoDB is known for its scalability, as it can handle massive workloads and is a distributed serverless database meaning there is no need to provision servers.
Replication across three availability zones for high availability.
Scales to millions of requests per second, trillions of rows, and hundreds of terabytes of storage.
Fast and consistent performance, with single-digit millisecond latency.
Integrated with IAM for security, authorization, and administration.
Low cost and auto scaling capabilities. Offers a standard and infrequent access IA table class for cost savings.
If you want to cache frequently read objects from DynamoDB tables, it is recommended to use DynamoDB Accelerator (DAX) instead of Elasticache. DAX provides better integration and performance specifically for DynamoDB, resulting in significantly improved latency and overall application performance.
DynamoDB Global Tables provide a way to replicate a DynamoDB table across multiple regions, allowing users to read and write to the table with low latency in their respective regions. The active-active replication feature ensures that changes made in any region are replicated to other regions, maintaining data consistency.

Redshift -
Redshift is a database that is based on PostgreSQL, but it is not used for OLTP (online transaction processing) like RDS. Instead, Redshift is designed for OLAP (online analytical processing), which is used for analyzing data and making computations.
It can scale to handle petabytes of data. Data in Redshift is stored in columns, making it a columnar storage of data.
Redshift uses a massively parallel query execution (MPP) engine to perform computations quickly.
It is integrated with business intelligence (BI) tools such as Quicksight or Tableau, allowing you to create dashboards on top of your data in the data warehouse.

EMR -
Elastic MapReduce is a service provided by AWS that allows users to create and manage Hadoop clusters for big data analysis. A Hadoop cluster is a group of computers (EC2 instances) that work together to analyze and process vast amounts of data. EMR allows users to create a cluster made up of hundreds of EC2 instances that collaborate to analyze data. It simplifies the provisioning and configuration of EC2 instances in the cluster and supports autoscaling and integration with spot instances. EMR is commonly used for data processing, machine learning, web indexing, and other big data applications.

Athena -
Athena is a serverless query service provided by AWS that allows you to perform analytics on objects stored in Amazon S3. It enables you to query files in S3 using SQL without the need to load them into a separate database. Athena is built on the Presto engine, which is a distributed SQL query engine.
It supports various file formats such as CSV, JSON, Orc, Avro, and Parquet.
If desired, you can use tools like Amazon QuickSight for getting reporting on top of Athena.

Quicksight -
Quicksight is a serverless, machine learning powered business intelligence service offered by AWS. It allows users to create interactive dashboards to visually represent data and provide insights to business users.
It offers a variety of features, including the ability to create graphs, charts, and other visualizations.
Quicksight is designed to be fast, automatically scalable, and embeddable into other applications.
Quicksight integrates with various AWS services, such as RDS databases, Aurora, Athena, Redshift, and S3.

DocumentDB -
DocumentDB is a NoSQL database service provided by AWS. It is based on MongoDB technology and is compatible with MongoDB.
DocumentDB is used to store, query, and index JSON data.
It is a fully managed database, similar to Aurora, and provides high availability.
Data in DocumentDB is replicated across three availability zones.
Storage in DocumentDB automatically grows in increments of 10GB, up to a maximum of 64 terabytes.
DocumentDB is engineered to handle workloads with millions of requests per second.

Neptune -
Neptune is a fully managed graph database that is designed to handle highly connected data sets, such as social networks. In a graph data set, entities are represented as nodes, and the relationships between entities are represented as edges. For example, in a social network, users are connected to each other through friendships, likes, comments, and shares.

QLDB & Amazon Managed Blockchain -
Quantum Ledger Database is a fully managed, serverless database that serves as a ledger for financial transactions. It allows you to review the history of all changes made to your application data over time. QLDB is designed to be immutable, meaning that once data is written to the database, it cannot be removed or modified. This ensures the integrity and security of financial transactions.

Amazon Managed Blockchain is a service provided by Amazon Web Services (AWS).
Blockchain is a technology that allows multiple parties to execute transactions without the need for a trusted central authority.
It allows users to join public blockchain networks or create their own private blockchain networks within the AWS environment.
Currently, it supports two blockchain frameworks: Hyperledger Fabric and Ethereum.

Difference between QLDB and Amazon Managed Blockchain
QLDB is a ledger database, while Amazon Managed Blockchain is a blockchain technology.
QLDB does not have a concept of decentralization, as it is a centralized database owned by Amazon. On the other hand, Amazon Managed Blockchain has decentralization.

Glue -
Glue is a managed Extract, Transform, and Load (ETL) service provided by Amazon Web Services. It allows you to prepare and transform your data sets for analytics by extracting data from various sources, transforming it, and loading it into different destinations. Glue is a fully serverless service, meaning you don't have to worry about managing servers and can focus solely on data transformation.
Glue Data Catalog -
The Glue Data Catalog is another component of AWS Glue.
It serves as a catalog or repository for your data sets within your AWS infrastructure.
The Glue Data Catalog contains metadata about your data sets, including column names, field names, and field types.
Services like Athena, Redshift, and EMR can utilize the Glue Data Catalog to discover and understand the structure of your data sets, enabling them to build appropriate schemas for analysis.

DMS -
Database Migration Service is used to transferring data from one database to another. It supports homogeneous & heterogeneous migrations.

--> Other Compute Services
Docker -
Docker is a software development platform that allows you to deploy applications by packaging them into containers. These containers are special because they can be run on any operating system with ease. Unlike traditional deployment methods where applications are installed directly on a specific operating system, Docker containers provide compatibility across different environments, resulting in predictable behavior and easier maintenance and deployment.
When using Docker on an EC2 instance, you can have multiple containers running different applications on the same instance.
Docker Images and Repositories -
To run a Docker container, you need to create a Docker image. An image is a lightweight, standalone, and executable package that includes everything needed to run the application, including the code, runtime, libraries, and dependencies. Docker images can be stored in Docker repositories.
There are two types of Docker repositories:
Public repositories: The Docker Hub is a popular public repository where you can find base images for various technologies and operating systems.
Private repositories: Amazon Elastic Container Registry (ECR) is a private Docker repository provided by AWS. This is particularly useful when you want to keep your images private or restrict access to specific users or AWS accounts.
Docker vs Virtual Machines -
In a traditional virtual machine setup (such as with EC2 instances on AWS), each virtual machine runs its own full operating system, which can be resource-intensive.
With Docker, multiple containers can run on a single host operating system, sharing resources and reducing overhead.
Docker containers are more lightweight and start up faster compared to virtual machines.
Docker containers are more portable and can be easily moved between different environments, whereas virtual machines are tied to specific hardware configurations.

ECS, Fargate, ECR -
ECS:
ECS stands for Elastic Container Service and is used to launch Docker containers on AWS.
With ECS, you need to provision and maintain the infrastructure yourself, which means creating EC2 instances in advance.
AWS takes care of starting or stopping the containers for you and provides integration with an Application Load Balancer.
In a web application scenario, multiple EC2 instances are created in advance, and ECS places the Docker containers on these instances.
ECS is a good choice when you want to run Docker containers on AWS and have control over the underlying infrastructure.
Fargate:
Fargate is also used to launch Docker containers on AWS, but it does not require provisioning(managing) any infrastructure.
Fargate is a serverless offering, meaning you don't need to manage any servers. AWS runs the containers based on the specified CPU and RAM requirements.
Fargate is simpler to use compared to ECS because you don't have to create or manage EC2 instances.
When you need to run a new Docker container on Fargate, AWS automatically runs it for you without specifying the exact location.
Fargate is a convenient option when you want to run Docker containers on AWS without the hassle of managing infrastructure.
ECR:
Elastic Container Registry is a private Docker registry service offered by AWS. It allows you to store your Docker images securely and efficiently. Here's how it works:
You upload your Docker images to ECR.
ECR stores these images in a secure and highly available manner.
You can then use these images to run containers using AWS services like ECS (Elastic Container Service) or Fargate.

Serverless Computing -
Serverless computing is a paradigm in which developers do not have to manage servers. Instead, they can focus on deploying their code or functions. Initially, serverless was pioneered as a function-as-a-service with AWS Lambda but now also includes anything that's managed: database, messaging, storage etc. However, nowadays, the term "serverless" is used to refer to any managed service that does not require the user to manage or provision servers. Some examples of serverless are S3, DynamoDB, Fargate, Lambda etc.

Lambda -
Lambda is a serverless computing service provided by Amazon Web Services (AWS). It allows you to run your code without provisioning or managing servers. It's important to note that while Lambda can run Docker containers using the Lambda Container Image, it requires the container image to implement the Lambda Runtime API. CloudWatch Events or EventBridge will trigger the Lambda function every hour (or based on the defined schedule) to perform a task. This allows you to run scripts periodically without the need for servers. It is like a cron job in Linux.
Traditional Server vs AWS Lambda -
Traditional Server (EC2 Instance):
Virtual server in the cloud.
Bounded by the amount of memory and CPU power allocated.
Continuously running, even when not in use.
Scaling requires adding or removing servers, which can be slow and complicated.
AWS Lambda:
Virtual functions instead of servers.
Functions are limited by time and intended for shorter executions.
Functions run on demand and are not billed when not in use.
Scaling is automated as part of the Lambda service.
Benefits of AWS Lambda -
Integration with whole AWS suite of services and many programming languages.
Event-Driven.
Easy monitoring through CloudWatch, which is the monitoring solution in AWS.
Can allocate up to 10GB of RAM per function which means resource flexibility.

API Gateway -
The API Gateway is a service provided by AWS that allows developers to easily create, publish, maintain, monitor, and secure APIs in the cloud. It is a fully managed service and is designed to work seamlessly with other AWS services like AWS Lambda and DynamoDB.
When building a serverless API, the Amazon API Gateway is an essential component to consider. It acts as a bridge between the client and the serverless functions, providing a secure and scalable way for clients to interact with your application.

Batch -
AWS Batch is a fully managed batch processing service that allows you to perform batch processing at any scale. It enables you to efficiently run hundreds of thousands of computing batch jobs on AWS with ease.
A batch job is a job that has a specific start and end time. Unlike continuous or streaming jobs that run indefinitely, batch jobs have a defined duration. For example, a batch job may start at 01:00 a.m. and finish at 03:00 a.m.
AWS Batch dynamically launches EC2 instances or spot instances to handle the load of running batch jobs. It automatically provisions the appropriate amount of compute and memory resources to handle your batch queue. This allows you to focus on submitting or scheduling batch jobs into the batch queue, while AWS Batch takes care of the infrastructure.
To define a batch job, you simply need a Docker image and a task definition that runs on the ECS (Elastic Container Service) service.

Lightsail -
Lightsail is a standalone service in AWS that provides virtual servers, storage, databases, and networking in one place. It offers low and predictable pricing, making it a simpler alternative to other AWS services like EC2, RDS, ELB, EBS, and Route 53.
Limitations of this are limited AWS integrations, no auto scaling and limited high availability.