--> Cloud Computing
Cloud computing is the on-demand delivery of compute power, database, storage, application, and other IT resources. It allows users to access these resources instantly, without the need for advance ordering or building their own data centers. Here are some key points about cloud computing:
On-demand delivery: Cloud computing provides resources when they are needed, allowing users to scale up or down as required.
Pay-as-you-go pricing: Users only pay for the resources they use, when they use them. There is no need for upfront investment or long-term commitments.
Provisioning: Cloud service platforms allow users to provision the exact type and size of computing resources they need. Whether it's a big server or a small one, the cloud can accommodate the user's requirements.
Instant access: Resources can be accessed instantly, without the need for 24 or 2 hours notice. Users can have access to servers, storage, databases, and application services within seconds.
Interface: Cloud computing provides a user-friendly interface that allows easy access to servers, storage, databases, and application services. This interface makes it convenient for users to manage and utilize their resources.

Types of Cloud Computing:
When you create your own server and databases, this is known as on-premises and you have to manage all the mentioned things like Application, Data, Runtime, Middleware, O/S, Virtualization, Servers, Storage and Networking etc.
1. Infrastructure as a Service (IaaS)
Provides the fundamental building blocks for cloud computing.
Offers networking, servers, virtualization and data storage.
Provides a high level of flexibility and allows easy migration from traditional on-premises IT to the cloud.
Examples of IaaS services include Amazon EC2, Google Cloud, Azure, Rackspace, and Digitalocean.

2. Platform as a Service (PaaS)
Removes the need for organizations to manage the underlying infrastructure.
Focuses on the deployment and management of applications.
Allows developers to concentrate on developing and running their applications without worrying about infrastructure management.
AWS manages everything from the runtime to the networking.
Examples of PaaS services include AWS Elastic Beanstalk, Google App Engine, and Microsoft Azure App Service.

3. Software as a Service (SaaS)
Provides a complete product that is run and managed by the service provider.
Users can access the software through the internet without the need for installation or maintenance.
Eliminates the need for organizations to manage software updates, security, and infrastructure.
Examples of SaaS services include Salesforce, Google Workspace, Microsoft Office 365, and Dropbox.

Types of Clouds:
1. Private Cloud
Used by a single organization and not exposed to the public.
Examples: Rackspace
Provides a private data center managed by a third party.
Offers complete control and more security for sensitive applications.

2. Public Cloud
Owned and operated by a third-party cloud service provider.
Resources are delivered over the internet.
Examples: Microsoft Azure, Google Cloud, Amazon Web Services (AWS).

3. Hybrid Cloud
Hybrid cloud combines private and public cloud infrastructure.
Some servers are kept on-premises while certain capabilities are extended to the cloud.
Provides control over sensitive assets in the private infrastructure.
Offers flexibility and cost-effectiveness of the public cloud.

--> IAM(Identity & Access Management)
It is a Global service in which we create our users and assign them to groups. Groups only contains users, not other groups. Users don't have to belong to a group, and user can belong to multiple groups. We create users or groups because we want to allow them to use our AWS accounts and we have to give them permissions. We add inline policies to those users who are not in any group. We protect users and groups by defining the password policy. The second option is using Multi Factor Authentication(MFA) which is password + security device you own to login the account. MFA devices options are - Virtual MFA device, Universal 2nd Factor Security Key etc.

Permissions -
Users or Groups can be assigned JSON documents called policies. These polices define the permissions of the users. In AWS you apply the least privilege principle that is don't give more permissions than a user needs.

IAM Policies Structure -
Version: policy language version
Id: an identifier for the policy (optional)
Statement: one or more individual statements (required)
Sid: an identifier for the statement (optional)
Effect: whether the statement allows or denies access
Principal: account/user/role to which this policy applied to
Action: list of actions or set of API calls this policy allows or denies based on the Effect
Resource: list of resources to which the actions applied to
Condition: conditions for when this policy is in effect (optional)

IAM Roles for Services -
Some AWS services will need to perform actions on your behalf. To do so, we'll assign permissions to AWS services with IAM Roles. Roles allows entities in the AWS to get credentials for a short duration and to do whatever they need to.

--> EC2
Elastic Compute Cloud is a service provided by AWS that allows users to rent virtual servers in the cloud, known as EC2 instances, and use them as compute resources. EC2 instances are bound to an Availibility Zones(AZ).
EC2 is composed of several components -
EC2 instances: Virtual machines that can be rented from AWS.
EBS volumes: Virtual drives used for storing data.
Elastic Load Balancer: Distributes load across multiple EC2 instances.
Auto Scaling Groups (ASG): Allows for scaling of services based on demand.

Choosing EC2 Instances:
When creating an EC2 instance, users have several options to choose from:
Operating System: Linux, Windows, or Mac OS.
Compute Power: Users can select the desired CPU and number of cores.
Random Access Memory (RAM): Users can choose the amount of RAM required.
Storage Space: Users can choose between network-attached storage (EBS or EFS) or hardware-attached storage (EC2 instance store).
Network Configuration: Users can select the type of network card and public IP.
Security Group: Users can define firewall rules for their EC2 instance.
Bootstrap Script (EC2 User Data): A script that configures the instance during its initial launch.

Instance Types -
Instances are categorized into families based on their intended use cases and performance characteristics.
Choosing the right instance type is important to ensure optimal performance for your application.
Some common instance families include:
General Purpose (e.g., t2, m5)
Compute Optimized (e.g., c5, r5)
Memory Optimized (e.g., x1, z1d)
Storage Optimized (e.g., i3, d2)
Naming convention:
m5.2xlarge (m is instance class, 5 is generation, 2xlarge is the size within the instance class)

EC2 User Data and Bootstrapping -
Bootstrapping refers to the process of launching commands or scripts when a machine starts.
The EC2 User Data script is run only once, during the initial boot of the instance.
The purpose of the EC2 User Data script is to automate boot tasks, hence the name bootstrapping.
The tasks you can automate using the User Data script can include installing updates, software, or downloading files from the internet.
The EC2 User Data script runs with root user privileges, so any commands you include will have sudo rights.

Security Groups -
Security groups are a fundamental aspect of network security in the AWS cloud.
Security groups act as firewalls around EC2 instances that determine what traffic is allowed to enter and exit the EC2 instances.
Security groups can have rules that reference either IP addresses or other security groups. Rules define whether inbound traffic from the outside to the EC2 instance is allowed, and if the EC2 instance can perform outbound traffic to the internet.
Security groups can be attached to multiple EC2 instances. There is not a one-to-one relationship between security groups and instances.
Similarly, an EC2 instance can have multiple security groups attached to it.
Instances are associated with security groups, and security groups are associated with rules.

Impact of Region and VPC Changes:
Security groups are tied to a specific region and VPC combination.
If you switch to another region or create a new VPC, you need to create new security groups.
This means that security groups are not portable across regions or VPCs.
It is important to consider this when planning and managing your security groups.

Troubleshooting Security Group Issues:
If you are unable to connect to an EC2 instance on a specific port and your computer hangs without any response, it is likely a security group issue. This is because the security group is blocking the inbound traffic to that port. On the other hand, if you receive a "connection refused" error, it means that the security group allowed the traffic to reach the instance, but the application running on the instance is not responding or not launched properly.
By default, all inbound traffic is blocked and all outbound traffic is authorized.
This means that you can initiate outbound connections from your instances, but incoming connections are not allowed unless explicitly configured in the security group rules.

Referencing Security Groups from Other Security Groups:
AWS provides a feature that allows you to reference one security group from another security group. This is particularly useful when working with load balancers. By referencing a security group in the inbound rules of another security group, you can allow instances associated with the second security group to communicate with instances associated with the first security group. This eliminates the need to specify IP addresses and simplifies the configuration process.
For example, if you have an EC2 instance with security group "Group 1" and you authorized inbound traffic from "Group 2" then any instance associated with "Group 2" can communicate with the instance associated with "Group 1" on the specified port.

SSH Access -
SSH access is often the most critical and sensitive aspect of security. 
It is recommended to maintain a separate security group specifically for SSH access.
This helps ensure that SSH access is properly configured and secured.
By dedicating a separate security group, it becomes easier to manage and monitor SSH traffic.
SSH(Secure Shell) allows you to control a remote machine or server using CLI. We create a SSH key to connect to the server or EC2 instance to maintain or perform some action on servers from CLI.
Linux and windows command - ssh -i '.\EC2 Tutorial.pem' ec2-user@65.2.57.62 (.pem is ssh key, 65.... is public IP address)

EC2 Instance Connect -
It opens a CLI in the AWS which connect the EC2 instance without the need of SSH. It needs the security group which have the SSH port access to make it work. Don't configure this with the aws configure command and by entering the access key ID and secret access key because the other users can also get your access keys information, so we do this giving by assigning the proper IAM roles to access some information.

--> EC2 Instance Storage
EBS volumes -
Elastic Block Store (EBS) is a block-level storage service provided by AWS. It allows you to create and attach persistent block storage volumes to your EC2 instances. They allow data to persist even after the instance is terminated.
Each EBS volume can only be mounted to one instance at a time.
When creating EBS volumes, you need to specify the AZ in which the volume will be created & it is bound to that specific availability zone.
EBS volumes are network drives, not physical drives and are attached to instances through the network.
Communication between the instance and the EBS volume is done through the network, which may introduce some latency.
EBS volumes can be quickly detached from one EC2 instance and attached to another.
EBS volumes are locked to specific availability zones, meaning they can only be attached to instances in the same availability zone.
When creating EBS volumes through EC2 instances, there is an attribute called "delete on termination". This attribute determines whether the EBS volume should be deleted automatically when the associated EC2 instance is terminated.
By default:
The "Delete on Termination" option is ticked for the root volume.
The "Delete on Termination" option is not ticked for any other attached EBS volumes.

EBS Snapshots -
It makes a backup of your EBS volume at a point of time. It is not necessary to detach volume to do snapshot. Can copy snapshots across AZ or regions, so that you would be able to transfer some of your data in a different region.

AMI -
AMI stands for Amazon Machine Image. AMIs represent a customization of an EC2 instance.
AMIs can be created by AWS or customized by users. AMIs contain software configurations, including the operating system and monitoring tools.
Behind the scenes, when we create an AMI, EBS snapshots are also created.
These snapshots capture the data stored on the instance's EBS volumes.
They provide a backup of the instance's data and can be used to restore or create new volumes.
Benefits of Using AMI:
Faster boot time and configuration time.
Prepackaged software installation on EC2 instances.
Can be built for a specific region and copied across regions. Allows for leveraging the AWS global infrastructure.

Types of AMIs:
1. Public AMIs
Provided by AWS. Can be used by all AWS users. No need to create or maintain them.
Examples: Amazon Linux 2 AMI.

2. Custom AMIs
Created and maintained by users. Can be built for specific requirements. Automation tools available for AMI creation. Users are responsible for creating and maintaining them.

3. AWS Marketplace AMIs
Created by vendors and potentially sold by them. Can save time by using pre-configured software. Users can also create and sell their own AMIs on the AWS Marketplace.

AMI Creation Process:
AMIs are a powerful tool for creating customized EC2 instances with pre-configured software and settings.
Launching an Instance in us-east-1a:
We will start by launching an instance in the us-east-1a region.
This instance will serve as the basis for our custom AMI.

Creating a Custom AMI:
Once we have customized the instance, we will create an AMI from it.
This AMI will capture the configuration and state of the instance at the time of creation.
It will serve as a template for launching new instances with the same configuration.

Launching Instances from the Custom AMI in us-east-1b:
In the us-east-1b region, we will be able to launch instances from the custom AMI we created.
This will effectively create a copy of our EC2 instance in a different region. We can launch multiple instances from the same AMI if needed.

EC2 Image Builder -
EC2 Image Builder is a service provided by AWS that allows you to automate the creation, maintenance, validation, and testing of Amazon Machine Images (AMIs) for EC2 instances. This service simplifies the process of building and customizing software components on EC2 instances, ensuring that your AMIs are up-to-date and meet your specific requirements.

EC2 Image Builder Working:
Setup: You configure and set up the EC2 Image Builder service, specifying the desired components and customizations for your AMIs.
Builder EC2 Instance: When the EC2 Image Builder service runs, it automatically creates a builder EC2 instance. This instance is responsible for building components and customizing the software according to your specifications. For example, it can install Java, update the CLI, update the software system, install firewalls, or even install your application.
AMI Creation: Once the builder EC2 instance completes its tasks, EC2 Image Builder creates an AMI from that instance. This AMI represents the desired state of your EC2 instances, including all the customizations and software installations.
Validation: To ensure the quality and functionality of the AMI, EC2 Image Builder automatically creates a test EC2 instance from the newly created AMI. You can define a set of tests to be run on this instance, such as checking if the AMI is working correctly, if it is secure, or if your application is running properly. You have the flexibility to skip certain tests if needed.
Distribution: Once the AMI passes the validation tests, it is ready for distribution. Although EC2 Image Builder is a regional service, you can distribute the AMI to multiple regions, allowing your application and workflow to be globally available.

EC2 Instance Store -
The EC2 Instance Store is a type of hardware disk that can be attached directly to an EC2 instance. It provides extremely high disk performance and is a great choice when you need better I/O performance and good throughput.
The storage provided by the EC2 Instance Store is considered ephemeral, meaning that if you stop or terminate the EC2 instance, the data stored on the instance store will be lost. Therefore, it is not suitable for long-term storage.
The EC2 Instance Store is ideal for use cases where you need temporary storage or scratch data. It can be used as a buffer or cache for storing data that is not critical or does not need to be persisted long-term.

EFS -
EFS stands for Elastic File System and is a managed network file system.
It allows you to attach a network file system to multiple EC2 instances simultaneously.
Unlike EBS volumes, which can only be attached to one instance at a time, EFS can be mounted onto hundreds of instances.
EFS is a shared network file system, also known as a shared NFS (Network File System).
It is designed to work with Linux EC2 instances and this architecture allows for easy sharing and access to files across instances in different availability zones.

--> Elastic Load Balancing & ASG
Scalability and High Availability -
Scalability allows an application to handle greater loads by adapting.
There are two types of scalability in the cloud:
1. Vertical scalability
Increasing the size or capacity of an instance. Example: Changing from t2.micro to t2.large.
Vertical scalability is common in non-distributed systems like databases.

2. Horizontal scalability (elasticity)
Increasing the number of instances. Improves performance and capacity. Increase fault tolerance and availability.
In AWS, achieving horizontal scalability:
Using Elastic Load Balancing (ELB) to distribute traffic across multiple instances.
Auto Scaling Groups (ASG) to automatically add or remove instances based on demand.
Horizontal scalability is commonly used in distributed systems to handle large workloads.

Scalability is linked but different from high availability.
High availability refers to the ability of an application to remain operational and accessible even in the face of failures or disruptions. It is recommended to use at least two availability zones for high availability.
Achieving high availability in AWS:
Using Elastic Load Balancing (ELB) to distribute traffic across multiple instances.
Auto Scaling Groups (ASG) to automatically replace failed instances.
Deploying resources across multiple Availability Zones (AZs) to ensure redundancy.

Scalability vs Elasticity vs Agility:
Scalability: Ability for a system to accommodate a larger load by making the hardware stronger (scaling up) or adding nodes (scaling out).
Elasticity: System's ability to automatically scale based on the load it receives.
Agility: New IT resources are only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes.

Elastic Load Balancing -
Elastic Load Balancing (ELB) is a service provided by AWS that allows for better scalability and availability of backend EC2 instances and they can be spread out across multiple AZ. It acts as a server that forwards internet traffic to multiple downstream servers, which are the backend EC2 instances. Load balancers handle upgrades, maintenance, and high availability, while users only need to configure their behavior.
Types of Load Balancers:
1. Application Load Balancer (ALB)
Layer 7 load balancer for HTTP and HTTPS traffic.
Supports HTTP routing features and static DNS for a static URL.
Simple architecture: Users access the load balancer using HTTP or HTTPS protocols.

2. Network Load Balancer (NLB)
Ultra-high performance load balancer.
Layer 4 load balancer for TCP and UDP traffic.
Suitable for applications that require low latency and high throughput.
Simple architecture: Users access the load balancer using TCP or UDP protocols.

3. Gateway Load Balancer (GWLB)
Layer 3 load balancer.
It is designed to route traffic to third-party security virtual appliances, such as firewalls, intrusion detection systems, or deep packet inspection systems.
GWLB allows you to inspect IP packets and perform firewall operations before forwarding the traffic to your applications.
It supports the Geneve protocol for routing traffic to virtual appliances running on EC2 instances.
GWLB architecture involves sending traffic to EC2 instances running virtual appliances, analyzing the traffic, and then forwarding it back to the load balancer for further routing to the applications.

Auto Scaling Groups -
An autoscaling group (ASG) is a feature provided by cloud service providers, that allows for the automatic creation and termination of instances based on the current demand or load on an application. ASGs are commonly used in conjunction with load balancers to ensure efficient and scalable application deployments. We can spread our load across multiple AZ.
The goal of ASG is to:
Scale out (add EC2 instances) to match an increased load.
Scale in (remove EC2 instances) to match a decreased load.
Ensure we have a min and max number of machines running.
Replace unhealthy instances.

ASG in AWS with Load Balancer:
When using an autoscaling group with EC2 instances, the load balancer plays a crucial role in distributing traffic.
Initially, if there is only one EC2 instance in the autoscaling group, the load balancer will redirect all incoming web traffic to that instance.
As the autoscaling group scales out by adding more EC2 instances, the load balancer will register them and distribute traffic across all instances.
The load balancer dynamically adjusts the distribution of traffic based on the number of instances available.
This ensures that the workload is evenly distributed and can handle increased traffic.
The load balancer can distribute traffic to the maximum size of the autoscaling group if it scales to that point.

--> S3
Amazon S3 stands for Simple Storage Service and is a cloud storage service provided by Amazon Web Services (AWS).
It allows you to store and retrieve large amounts of data, such as files, documents, images, videos, etc. It offers a wide range of use cases, including backup and storage, disaster recovery, archival purposes, hybrid cloud storage, hosting applications and media, data lakes, big data analytics, delivering software updates, and hosting static websites.
Files in Amazon S3 are called objects.
Files in Amazon S3 are stored in buckets, which can be seen as top-level directories.
Buckets are created in your account and must have a globally unique name.
S3 looks like a global service but buckets are created in a region.
S3 pre-signed URL contains a signature that verifies the credentials in the encoded URL to allow the user to show its object. 
S3 Objects:
Objects in S3 are files.
Each object has a key, which is the full path of the file.
The key of a file in the top-level directory is the same as the object name.
If a file is nested in folders, the key is the full path of the file. The key is composed of a prefix (folder path) and an object name.
S3 does not have a concept of directories, everything is a key.
Objects in Amazon S3 can have metadata associated with them. Metadata consists of a list of key-value pairs that provide additional information about the object. Metadata can be set by the system or by the user.
Each object can have Tags (up to 10 Unicode key-value pairs as metadata which are useful for security/lifecycle).
Object can have Version ID (if versioning is enabled).
Object Values:
The values of S3 objects are the content of the file.
You can upload any type of file to S3. There is a maximum size limit for objects in S3.

S3 Security -
1. User-Based Security
IAM policies determine which API calls are allowed for a user. IAM principles can access S3 objects if:
IAM permissions allow it.
Resource policies allow it.
There is no explicit deny in the action.

2. Resource-Based Security
(i) Bucket Policies
S3 bucket policies are bucket-wide rules that can be assigned from the S3 console.
Bucket policies allow specific users or users from other accounts (cross-account) to access the S3 bucket.
Bucket policies can be used to make S3 buckets public.
(ii) Object Access Control List (ACL)
ACL provides finer-grained security for S3 objects. ACL can be disabled.
(iii) Bucket-level ACL is less common but can also be disabled.

3. Object Encryption
Objects in S3 can be encrypted using encryption keys.

Bucket Policies -
Bucket policies are the most common way to secure an S3 bucket. Bucket policies are JSON-based policies.
The structure of a bucket policy:
Resource: Specifies the buckets and objects the policy applies to.

Granting Public Access
To grant public access to a bucket or objects within it, we can use an S3 bucket policy.
For example, we can set a bucket policy that allows anyone (*) to perform the GetObject action on all objects within the bucket.
This would make all objects within the bucket publicly readable.

Granting Access to IAM Users
If we have an IAM user within our AWS account who needs access to the S3 bucket, we can assign IAM permissions to that user through a policy.
By creating a policy that allows access to the S3 bucket, the user will be able to access the bucket.

Granting Access from EC2 Instances
If we want to give access from an EC2 instance to an S3 bucket, IAM users are not appropriate. Instead, we need to use IAM roles.
We can create an IAM role for the EC2 instance with the necessary IAM permissions to access the S3 bucket.
By assigning this role to the EC2 instance, it will be able to access the S3 bucket.

Allowing Cross-Account Access
If we want to allow access from an IAM user in another AWS account, we can use a bucket policy.
We can create an S3 bucket policy that allows cross-account access.
By specifying the IAM user in the other AWS account in the bucket policy, we can grant them access to the S3 bucket.

Static Website Hosting, Versioning, Replication, S3 Storage Classes -
Easy topics, if you forgot these watch a quick tutorial.

AWS Snow Family -
The AWS Snow Family is a collection of highly secure portable devices that serve two main use cases within AWS: data migration and edge computing. AWS OpsHub is a software to connect & manage your Snow Family Devices.
Data migration refers to the process of transferring large amounts of data in and out of AWS. The AWS Snow Family offers three different types of devices for data migration: Snow Cone, Snowball Edge, and Snowmobile and each option has different storage capacities 8 terabytes, 80 terabytes, and 100 petabytes respectively.
Recommended migration size for Snow Cone is up to 24 terabytes online and offline and DataSync agent is pre-installed on this.
Snowball Edge is suitable for petabyte migrations and requires offline data transfer & supports storage clustering to put 15 of them together.
Snowmobile is the best choice for transferring more than ten petabytes of data.

Edge computing is a concept where data processing and analysis is performed closer to the source of data creation, rather than sending it back to the cloud. AWS Snow Family provides a range of devices that enable edge computing capabilities which are Snowcone, Snowcone SSD, Snowball Edge and they can run EC2 instances & AWS Lambda functions on them.

Storage Gateway -
It is a hybrid solution to extend on-premises storage to S3. Bridge b/w data stored on your end & cloud data in S3.
Summarizing the storage options on AWS:
Block storage will be EBS, EC2 instance store.
File storage will be EFS.
Object storage will be S3, Glacier.

--> Database Analytics
RDS -
RDS stands for Relational Database Service. It is a managed database service provided by AWS for relational databases.
RDS uses SQL as the query language. It allows you to create and manage databases in the cloud.
Supported databases include PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server, and Aurora (proprietary database from AWS).

Aurora -
Aurora is a database technology created by AWS. It is not open source and is designed to be cloud optimized.
Aurora is not included in the free tier of AWS. Aurora supports two types of databases: PostgreSQL and MySQL.
Aurora offers a 5x performance improvement over MySQL on RDS and a 3x performance improvement over PostgreSQL on RDS.
Storage for Aurora databases automatically grows in increments of 10GB up to 128 terabytes.
Aurora is more expensive than RDS (about 20% more), but it is more efficient and cost-effective.

RDS Database Deployment Options -
RDS Read Replicas -
Read replicas are used to scale the read workload of an RDS database.
By creating read replicas, copies of the main RDS database are created, allowing applications to read from these replicas.
This distributes the read workload across multiple RDS databases. Up to 15 read replicas can be created for an RDS database.
Writing data is still done only to the main database.
Multi-AZ Deployment -
Multi-AZ deployment is used to achieve high availability in case of an availability zone (AZ) outage.
A replication is set up across a different AZ, creating a failover database.
If the main RDS database crashes or the AZ experiences issues, RDS triggers a failover.
Data is only read and written to the main database.
The failover database remains passive and is not used for active read or write operations until there is a failure in the main DB.
Multi-Region Deployment -
In a multi-region deployment, read replicas of the RDS database are created in different regions.
Read replicas allow applications in different regions to read data locally, reducing latency.
However, writes still need to happen cross-region, which may introduce some latency.
This deployment is useful for setting up a disaster recovery strategy in case of a regional issue.
It provides better performance for applications in different regions.

Amazon ElastiCache -
Amazon ElastiCache is a type of database service provided by AWS. It is used to manage in-memory databases, specifically Redis or Memcached. These in-memory databases offer high performance and low latency.
The architecture for using ElastiCache involves an elastic load balancer directing traffic to EC2 instances. These instances read and write data from the main Amazon RDS database. If possible, they also cache certain values in the ElastiCache database. This helps in offloading the pressure from the main database and improving overall performance.

DynamoDB -
DynamoDB is a fully managed, highly available database provided by AWS. It is part of the NoSQL database family, meaning it is not a relational database. DynamoDB is known for its scalability, as it can handle massive workloads and is a distributed serverless database meaning there is no need to provision servers.
Replication across three availability zones for high availability.
Scales to millions of requests per second, trillions of rows, and hundreds of terabytes of storage.
Fast and consistent performance, with single-digit millisecond latency.
Integrated with IAM for security, authorization, and administration.
Low cost and auto scaling capabilities. Offers a standard and infrequent access IA table class for cost savings.
If you want to cache frequently read objects from DynamoDB tables, it is recommended to use DynamoDB Accelerator (DAX) instead of ElastiCache. DAX provides better integration and performance specifically for DynamoDB, resulting in significantly improved latency and overall application performance.
DynamoDB Global Tables provide a way to replicate a DynamoDB table across multiple regions, allowing users to read and write to the table with low latency in their respective regions. The active-active replication feature ensures that changes made in any region are replicated to other regions, maintaining data consistency.

Redshift -
Redshift is a database that is based on PostgreSQL, but it is not used for OLTP (online transaction processing) like RDS. Instead, Redshift is designed for OLAP (online analytical processing), which is used for analyzing data and making computations.
It can scale to handle petabytes of data. Data in Redshift is stored in columns, making it a columnar storage of data.
Redshift uses a massively parallel query execution (MPP) engine to perform computations quickly.
It is integrated with business intelligence (BI) tools such as Quicksight or Tableau, allowing you to create dashboards on top of your data in the data warehouse.

EMR -
Elastic MapReduce is a service provided by AWS that allows users to create and manage Hadoop clusters for big data analysis. A Hadoop cluster is a group of computers (EC2 instances) that work together to analyze and process vast amounts of data. EMR allows users to create a cluster made up of hundreds of EC2 instances that collaborate to analyze data. It simplifies the provisioning and configuration of EC2 instances in the cluster and supports autoscaling and integration with spot instances. EMR is commonly used for data processing, machine learning, web indexing, and other big data applications.

Athena -
Athena is a serverless query service provided by AWS that allows you to perform analytics on objects stored in Amazon S3. It enables you to query files in S3 using SQL without the need to load them into a separate database. Athena is built on the Presto engine, which is a distributed SQL query engine.
It supports various file formats such as CSV, JSON, Orc, Avro, and Parquet.
If desired, you can use tools like Amazon QuickSight for getting reporting on top of Athena.

Quicksight -
Quicksight is a serverless, machine learning powered business intelligence service offered by AWS. It allows users to create interactive dashboards to visually represent data and provide insights to business users.
It offers a variety of features, including the ability to create graphs, charts, and other visualizations.
Quicksight is designed to be fast, automatically scalable, and embeddable into other applications.
Quicksight integrates with various AWS services, such as RDS databases, Aurora, Athena, Redshift, and S3.

DocumentDB -
DocumentDB is a NoSQL database service provided by AWS. It is based on MongoDB technology and is compatible with MongoDB.
DocumentDB is used to store, query, and index JSON data.
It is a fully managed database, similar to Aurora, and provides high availability.
Data in DocumentDB is replicated across three availability zones.
Storage in DocumentDB automatically grows in increments of 10GB, up to a maximum of 64 terabytes.
DocumentDB is engineered to handle workloads with millions of requests per second.

Neptune -
Neptune is a fully managed graph database that is designed to handle highly connected data sets, such as social networks. In a graph data set, entities are represented as nodes, and the relationships between entities are represented as edges. For example, in a social network, users are connected to each other through friendships, likes, comments, and shares.

QLDB & Amazon Managed Blockchain -
Quantum Ledger Database is a fully managed, serverless database that serves as a ledger for financial transactions. It allows you to review the history of all changes made to your application data over time. QLDB is designed to be immutable, meaning that once data is written to the database, it cannot be removed or modified. This ensures the integrity and security of financial transactions.

Amazon Managed Blockchain is a service provided by Amazon Web Services (AWS).
Blockchain is a technology that allows multiple parties to execute transactions without the need for a trusted central authority.
It allows users to join public blockchain networks or create their own private blockchain networks within the AWS environment.
Currently, it supports two blockchain frameworks: Hyperledger Fabric and Ethereum.

Difference between QLDB and Amazon Managed Blockchain
QLDB is a ledger database, while Amazon Managed Blockchain is a blockchain technology.
QLDB does not have a concept of decentralization, as it is a centralized database owned by Amazon. On the other hand, Amazon Managed Blockchain has decentralization.

Glue -
Glue is a managed Extract, Transform, and Load (ETL) service provided by Amazon Web Services. It allows you to prepare and transform your data sets for analytics by extracting data from various sources, transforming it, and loading it into different destinations. Glue is a fully serverless service, meaning you don't have to worry about managing servers and can focus solely on data transformation.
Glue Data Catalog -
The Glue Data Catalog is another component of AWS Glue.
It serves as a catalog or repository for your data sets within your AWS infrastructure.
The Glue Data Catalog contains metadata about your data sets, including column names, field names, and field types.
Services like Athena, Redshift, and EMR can utilize the Glue Data Catalog to discover and understand the structure of your data sets, enabling them to build appropriate schemas for analysis.

DMS -
Database Migration Service is used to transferring data from one database to another. It supports homogeneous & heterogeneous migrations.

--> Other Compute Services
Docker -
Docker is a software development platform that allows you to deploy applications by packaging them into containers. These containers are special because they can be run on any operating system with ease. Unlike traditional deployment methods where applications are installed directly on a specific operating system, Docker containers provide compatibility across different environments, resulting in predictable behavior and easier maintenance and deployment.
When using Docker on an EC2 instance, you can have multiple containers running different applications on the same instance.
Docker Images and Repositories -
To run a Docker container, you need to create a Docker image. An image is a lightweight, standalone, and executable package that includes everything needed to run the application, including the code, runtime, libraries, and dependencies. Docker images can be stored in Docker repositories.
There are two types of Docker repositories:
Public repositories: The Docker Hub is a popular public repository where you can find base images for various technologies and operating systems.
Private repositories: Amazon Elastic Container Registry (ECR) is a private Docker repository provided by AWS. This is particularly useful when you want to keep your images private or restrict access to specific users or AWS accounts.
Docker vs Virtual Machines -
In a traditional virtual machine setup (such as with EC2 instances on AWS), each virtual machine runs its own full operating system, which can be resource-intensive.
With Docker, multiple containers can run on a single host operating system, sharing resources and reducing overhead.
Docker containers are more lightweight and start up faster compared to virtual machines.
Docker containers are more portable and can be easily moved between different environments, whereas virtual machines are tied to specific hardware configurations.

ECS, Fargate, ECR -
ECS:
ECS stands for Elastic Container Service and is used to launch Docker containers on AWS.
With ECS, you need to provision and maintain the infrastructure yourself, which means creating EC2 instances in advance.
AWS takes care of starting or stopping the containers for you and provides integration with an Application Load Balancer.
In a web application scenario, multiple EC2 instances are created in advance, and ECS places the Docker containers on these instances.
ECS is a good choice when you want to run Docker containers on AWS and have control over the underlying infrastructure.
Fargate:
Fargate is also used to launch Docker containers on AWS, but it does not require provisioning(managing) any infrastructure.
Fargate is a serverless offering, meaning you don't need to manage any servers. AWS runs the containers based on the specified CPU and RAM requirements.
Fargate is simpler to use compared to ECS because you don't have to create or manage EC2 instances.
When you need to run a new Docker container on Fargate, AWS automatically runs it for you without specifying the exact location.
Fargate is a convenient option when you want to run Docker containers on AWS without the hassle of managing infrastructure.
ECR:
Elastic Container Registry is a private Docker registry service offered by AWS. It allows you to store your Docker images securely and efficiently. Here's how it works:
You upload your Docker images to ECR.
ECR stores these images in a secure and highly available manner.
You can then use these images to run containers using AWS services like ECS (Elastic Container Service) or Fargate.

Serverless Computing -
Serverless computing is a paradigm in which developers do not have to manage servers. Instead, they can focus on deploying their code or functions. Initially, serverless was pioneered as a function-as-a-service with AWS Lambda but now also includes anything that's managed: database, messaging, storage etc. However, nowadays, the term "serverless" is used to refer to any managed service that does not require the user to manage or provision servers. Some examples of serverless are S3, DynamoDB, Fargate, Lambda etc.

Lambda -
Lambda is a serverless computing service provided by Amazon Web Services (AWS). It allows you to run your code without provisioning or managing servers. It's important to note that while Lambda can run Docker containers using the Lambda Container Image, it requires the container image to implement the Lambda Runtime API. CloudWatch Events or EventBridge will trigger the Lambda function every hour (or based on the defined schedule) to perform a task. This allows you to run scripts periodically without the need for servers. It is like a cron job in Linux.
Traditional Server vs AWS Lambda -
Traditional Server (EC2 Instance):
Virtual server in the cloud.
Bounded by the amount of memory and CPU power allocated.
Continuously running, even when not in use.
Scaling requires adding or removing servers, which can be slow and complicated.
AWS Lambda:
Virtual functions instead of servers.
Functions are limited by time and intended for shorter executions.
Functions run on demand and are not billed when not in use.
Scaling is automated as part of the Lambda service.
Benefits of AWS Lambda -
Integration with whole AWS suite of services and many programming languages.
Event-Driven.
Easy monitoring through CloudWatch, which is the monitoring solution in AWS.
Can allocate up to 10GB of RAM per function which means resource flexibility.

API Gateway -
The API Gateway is a service provided by AWS that allows developers to easily create, publish, maintain, monitor, and secure APIs in the cloud. It is a fully managed service and is designed to work seamlessly with other AWS services like AWS Lambda and DynamoDB.
When building a serverless API, the Amazon API Gateway is an essential component to consider. It acts as a bridge between the client and the serverless functions, providing a secure and scalable way for clients to interact with your application.

Batch -
AWS Batch is a fully managed batch processing service that allows you to perform batch processing at any scale. It enables you to efficiently run hundreds of thousands of computing batch jobs on AWS with ease.
A batch job is a job that has a specific start and end time. Unlike continuous or streaming jobs that run indefinitely, batch jobs have a defined duration. For example, a batch job may start at 01:00 a.m. and finish at 03:00 a.m.
AWS Batch dynamically launches EC2 instances or spot instances to handle the load of running batch jobs. It automatically provisions the appropriate amount of compute and memory resources to handle your batch queue. This allows you to focus on submitting or scheduling batch jobs into the batch queue, while AWS Batch takes care of the infrastructure.
To define a batch job, you simply need a Docker image and a task definition that runs on the ECS (Elastic Container Service) service.

Lightsail -
Lightsail is a standalone service in AWS that provides virtual servers, storage, databases, and networking in one place. It offers low and predictable pricing, making it a simpler alternative to other AWS services like EC2, RDS, ELB, EBS, and Route 53.
Limitations of this are limited AWS integrations, no auto scaling and limited high availability.

--> Deploying and Managing Infrastructure at Scale
CloudFormation -
CloudFormation is a technology in AWS that allows you to declaratively outline your AWS infrastructure for any resources.
It provides a way to define your infrastructure as code, specifying the desired configuration of resources.
With CloudFormation, you can create and manage your infrastructure in a consistent and automated manner.
Cloud Development Kit -
You can choose any supported language to write the code of your application and this is compiled into a CloudFormation template (JSON/YAML) using the CDK CLI and this template will be applied into CloudFormation to deploy our infrastructure.

Elastic Beanstalk -
Elastic Beanstalk is a developer-centric platform for deploying applications on AWS. It provides a simplified view of the underlying infrastructure and allows developers to focus on running their code rather than managing the infrastructure.
Features of Elastic Beanstalk -
Managed Service: Elastic Beanstalk handles the configuration and management of EC2 instances and the operating system, making it developer-friendly.
Capacity Provisioning: Elastic Beanstalk automatically provisions and manages the capacity of resources through an Auto Scaling group and load balancing.
Application Health Monitoring: Elastic Beanstalk provides a dashboard for monitoring the health and responsiveness of applications.
Architecture Models: Elastic Beanstalk supports three architecture models:
Single Instance Deployment: Suitable for development environments.
Load Balanced and Auto Scaled: Ideal for production or pre-production web applications.
Auto Scaled (non-web apps): Used for non-web applications in production, such as workers.

CodeDeploy (hybrid) -
CodeDeploy is a service provided by AWS that allows for automatic deployment of applications. It is a more flexible option compared to CloudFormation and Elastic Beanstalk, as it does not require the use of these services. CodeDeploy can be used independently to upgrade applications from one version to another.
Features of CodeDeploy -
Works with EC2 instances: CodeDeploy can upgrade multiple EC2 instances from version one to version two of an application.
Works with on-premises servers: CodeDeploy also supports upgrading applications on servers that are located on-premises. This makes it a hybrid service, as it can be used for both on-premises and EC2 instances.
Requires server provisioning: Before using CodeDeploy, servers need to be provisioned and configured. This includes installing the CodeDeploy agent on the servers, which assists in performing the upgrades.

CodeCommit -
CodeCommit is a code repository service provided by AWS. CodeCommit is based on the Git technology.
CodeCommit is integrated with all other AWS services. It allows developers to store their code in a version control repository.
It provides a fully managed, scalable, and highly available code repository.
The code repository is private and secure, as it lives within your AWS account.

CodeBuild -
CodeBuild is a service provided by AWS that allows you to build your code in the cloud. It takes your source code, compiles it, runs tests, and produces ready-to-deploy packages. It's fully managed, continuously scalable, highly available, secure and a serverless service. Here's an overview of how it works:
CodeBuild retrieves your code from CodeCommit, where it is stored.
It runs scripts that you define to build your code.
The output of the build process is a set of artifacts, which are ready to be deployed.

CodePipeline -
CodePipeline is a fully managed service provided by AWS that allows you to orchestrate and automate the different steps involved in deploying code to production. It serves as an orchestration layer that connects various AWS services together to create a pipeline for continuous integration and continuous delivery (CI/CD).
Benefits of CodePipeline -
Fully managed: CodePipeline is a fully managed service provided by AWS, which means that AWS takes care of the underlying infrastructure and maintenance, allowing you to focus on your code and application.
Compatibility: CodePipeline is compatible with various AWS services such as CodeCommit, CodeBuild, CodeDeploy, Elastic Beanstalk, as well as third-party services like GitHub.
Fast delivery and rapid updates: CodePipeline enables fast delivery and rapid updates by automating the entire deployment process.

CodeArtifact -
Software packages created by developers often depend on each other to be built, forming a code architecture.
Storing and retrieving these dependencies is known as artifact management.
Traditionally, developers had to set up their own artifact management system, which could be complex.
AWS CodeArtifact is a secure, scalable, and cost-effective artifact management service provided by AWS.
Benefits of AWS CodeArtifact -
Provides a secure and reliable place to store and retrieve code dependencies.
Supports common dependency management tools such as Maven, Gradle, npm, Yarn, Twine, Pip, and NuGet.
Integrates seamlessly with other AWS services like CodeCommit and CodeBuild.
CodeArtifact is useful for teams that require an artifact management system or a centralized location to store their code dependencies.
Developers can push their code to CodeCommit, and CodeBuild can directly retrieve the dependencies from CodeArtifact during the build process.

Codestar -
Codestar is a unified user interface (UI) that simplifies the management of software development activities in one place. It addresses the challenge of setting up and integrating various development tools such as CodeCommit, CodeBuild, CodeDeploy, CodePipeline, Elastic Beanstalk and EC2. Codestar provides a convenient solution by offering a one-stop shop for starting a development project and automatically creating the necessary resources behind the scenes.

Cloud9 -
Cloud9 is a cloud-based Integrated Development Environment (IDE) that allows developers to write, run, and debug code directly in the cloud. It provides a code editor that runs in a web browser, eliminating the need to download IDE. Cloud9 enables real-time code collaboration, allowing multiple developers to work on the same code simultaneously. This feature is particularly useful for pair programming or team projects.

SSM (hybrid) -
AWS Systems Manager is a hybrid AWS service that allows you to manage your fleet of EC2 instances and on-premises systems at scale. It provides operational insights about the state of your infrastructure and offers a suite of ten plus products to help you manage your resources efficiently.
Key Features -
Automated Patching: SSM allows you to automate the patching of all your servers and instances for enhanced compliance.
Run Commands: You can run commands across your entire fleet of servers directly from SSM, ensuring consistent execution.
Parameter Store: SSM provides a parameter store to store and manage configuration parameters.
Cross-Platform Compatibility: SSM works for Linux, Windows, macOS, and Raspberry Pi, making it versatile for different environments.
How SSM Works -
Install SSM Agent: To use SSM, you need to install the SSM agent on the systems you want to control.
Agent Reporting: The SSM agent reports back to the SSM service in AWS, providing information about the managed instances.
EC2 Instances and On-Premises VMs (Virtaul Machine): The SSM agent is installed on both EC2 instances and on-premises virtual machines, allowing control and management of both types of resources.
Troubleshooting: If an instance cannot be controlled by SSM, it is likely due to an issue with the SSM agent.
SSM Session Manager -
It allows you to start a secure shell on your EC2 and on-premises servers. It don't need SSH keys, no port 22 needed. Supported on Linux, macOS and Windows. It sends session log data to S3 or CloudWatch logs.

OpsWorks -
OpsWorks is a service provided by AWS that allows you to manage server configuration using tools like Chef and Puppet. Chef and Puppet are not created by AWS, but they are widely used for automating server configuration and repetitive actions. OpsWorks is designed to work seamlessly with EC2 instances or on-premises virtual machines. It serves as an alternative to AWS Systems Manager (SSM) and allows you to provision standard AWS resources such as EC2 instances, databases, load balancers, and EBS volumes.

--> AWS Global Infrastructure
Global Infrastructure -
A global application is deployed in multiple geographies, such as different AWS regions or edge locations.
Latency refers to the time it takes for a network packet to reach a server.
Deploying an application closer to users reduces latency and improves performance.
For example, if a user in India accesses a server in the US, there will be more latency compared to a server located in Asia.
Reasons for Creating a Global Application -
Decreased Latency:
Deploying an application in multiple regions allows users to access it from the nearest location.
Users around the world experience faster response times and better performance.
Reducing latency enhances the overall user experience.
Disaster Recovery:
Having a disaster recovery plan is crucial to ensure high availability of the application.
By deploying the application in multiple regions, if one region goes down due to natural disasters, power shutdowns, or other reasons, the application can fail over to another region.
This ensures that the application remains operational even in the event of a regional outage.
Protection Against Attacks:
Online attackers may target applications to disrupt their functionality.
Distributing the application across multiple regions makes it more challenging for attackers to bring down the entire application.
If one region is under attack, the application can still function from other regions, providing resilience against attacks.

AWS Regions:
AWS regions are geographical locations where AWS has data centers.
Each region is independent and isolated from other regions.
Regions are spread across the world, but not every country has a region.
Regions are identified by names such as Spain, London, Ireland, Paris, Frankfurt, Milan, and Stockholm.
Availability Zones (AZ) -
Each region is divided into multiple availability zones (AZ).
Availability zones are physically separate data centers within a region.
AZs are designed to be isolated from each other to ensure fault tolerance and high availability.
AZs within a region are connected by a fast network.
Edge Locations -
Edge locations, also known as points of presence (PoP), are used for content delivery.
Edge locations are strategically located around the world to be closer to end users.
They are used to cache and deliver content to users with low latency.
They are not the same as regions or availability zones.

Route 53 -
Amazon Route 53 is a managed DNS (Domain Name System) service.
DNS is like a phone book that helps clients find the right servers through URLs.
Route 53 is important for deploying a global application.
DNS Records in AWS -
A record: Maps a domain name to an IPv4 address.
AAAA record: Maps a domain name to an IPv6 address.
CNAME record: Maps a hostname to another hostname.
Alias record: Maps a hostname to an AWS resource (e.g., ELB, CloudFront, S3, RDS).
Understanding DNS with Route 53 -
Web browser wants to access "myapp.mydomain.com".
Application server has a public IPv4 address.
To enable access using the URL, an A record is created in Route 53.
When the web browser sends a DNS request for "myapp.mydomain.com", Route 53 replies with the IP address.
The web browser uses the IP address to reach the correct server and get the HTTP response.

Route 53 Routing Policies:
1. Simple Routing Policy
No health checks
Web browser performs DNS query and receives an IPv4 address
Basic routing policy

2. Weighted Routing Policy
Distributes traffic across multiple EC2 instances
Assign weights to EC2 instances (e.g., 70, 20, 10)
DNS ensures clients receive specified percentages of traffic to each instance
Can use health checks

3. Latency Routing Policy
Used for globally distributed applications
Example: servers in California and Australia, users worldwide
Redirects users based on their location to minimize latency
Users close to America EC2 instance are directed to that server
Users close to Australia are directed to the Australia server
Route 53 minimizes latency between users and servers

4. Failover Routing Policy
Used for disaster recovery
Involves a primary EC2 instance and a failover instance
DNS system performs health check on primary instance
If primary instance fails, traffic is redirected to failover instance
Ensures clients know which instance to connect to in case of failure

Cloudfront -
Cloudfront is a content delivery network (CDN) provided by AWS. It improves the read performance of your website by caching its content at different edge locations around the world. This means that when a user requests your website, the content is served from the nearest edge location, reducing latency and improving the user experience.
One of the benefits of using Cloudfront is that it provides Distributed Denial of Service (DDoS) protection. DDoS attacks are a type of attack where multiple servers are simultaneously targeted.
CloudFront can also be used to upload data to an S3 bucket. This process is called ingress. You can have CloudFront in front of any custom origin HTTP backend, such as an Application Load Balancer, an EC2 instance, or an S3 website. However, before you can use CloudFront to upload data to an S3 bucket, you need to enable the bucket as a static S3 website or any other HTTP backend you prefer.
Origin Types -
S3 buckets: Used to distribute files and cache them at the edge locations. When a user requests content, Cloudfront can fetch it from the S3 bucket and cache it at the edge location for future requests. To ensure that only CloudFront can access your S3 bucket, you can use Origin Access Control (OAC). OAC replaces the older Origin Access Identity (OAI) feature. By configuring OAC, you can restrict access to your S3 bucket so that it can only be accessed through CloudFront. This adds an extra layer of security to your content.
Custom origins: These can be other AWS services, such as EC2 instances or Elastic Load Balancers, or even non-AWS resources. Cloudfront can fetch content from these origins and cache it at the edge locations.

Differences b/w CloudFront and S3 Replication:
CloudFront is a CDN that caches content globally, while S3 replication replicates an entire bucket into another region
CloudFront is ideal for static content, while S3 replication is suitable for dynamic content
CloudFront uses caching to serve content from edge locations, while S3 replication provides near real-time updates

S3 Transfer Acceleration -
S3 Transfer Acceleration is a feature provided by Amazon S3 that allows for faster file transfers between different S3 buckets located in different regions. It is particularly useful when you need to transfer files from all around the world into a specific S3 bucket.
How it works:
When you upload a file from a specific location (e.g., United States) to an S3 bucket in a different region (e.g., Australia), the file is first uploaded to an edge location that is geographically closer to the user (e.g., an edge location in the USA).
From the edge location, the file is then transferred to the S3 bucket in the target region (e.g., Australia) using Amazon's internal network, which provides a more reliable and faster connection.

AWS Global Accelerator -
AWS Global Accelerator is a service provided by Amazon Web Services (AWS) that aims to improve the global availability and performance of applications. It achieves this by leveraging the AWS global network and optimizing the routing of requests to applications.
Improves performance for a wide range of applications over TCP or UDP.
Good for HTTP use cases that require static IP addresses, deterministic and fast regional failover.
How it works:
Routing through the AWS network: When using AWS Global Accelerator, your requests are routed through the internal network of AWS. This allows for optimized routing to your application, resulting in a potential 60% improvement in performance.
Edge locations: Users from around the world can access your application by connecting to an edge location. The edge location then routes the traffic directly to the region where your application is deployed. This reduces the traffic on the public Internet, as it only occurs between the user's location and the closest edge location.
Leveraging the private AWS network: Once the traffic reaches the edge location, it is then sent through the private AWS network to your application load balancer. This private network connection is faster and more reliable than relying solely on the public Internet.
Anycast IP: Accessing your application is done through two static IPs called Anycast IP. These IPs automatically redirect users to the correct edge location based on their location. This ensures that users are always directed to the nearest edge location for optimal performance.

Difference b/w Global Accelerator and CloudFront:
They both utilize the global network of AWS and the edge locations around the world and they both integrate with AWS Shield for DDoS protection. However, there are some differences between the two services:
AWS Global Accelerator is primarily focused on improving the availability and performance of applications and there is no caching present, while CloudFront is a content delivery network (CDN) that is designed to deliver content, such as static files and media, with low latency and high transfer speeds.

AWS Outpost -
AWS Outpost is an exciting development that allows businesses to have a hybrid cloud infrastructure. A hybrid cloud refers to the combination of an on-premises infrastructure and a cloud infrastructure. AWS Outpost are "server racks" that offers the same AWS infrastructure services, APIs, and tools on their on-premises infrastructure as they do in the cloud.
How AWS Outpost Works:
AWS sets up and manages Outposts racks within your on-premises infrastructure & you can start leveraging AWS services on-premises.
These Outpost racks come preloaded with AWS services, APIs, and tools, allowing businesses to build and run applications on their on-premises infrastructure just like in the cloud.
However, it's important to note that with AWS Outpost, businesses are responsible for the physical security of the rack itself, as it is located within their own data center.
Benifits:
Fully managed service, Low latency access to on-premises systems, Local data processing, Data residency, Easy migration from on-premises to the cloud. EC2, EBS, S3, EKS, ECS, RDS and EMR are some services that works on Outposts.

AWS Wavelength -
AWS Wavelength is a feature of Amazon Web Services (AWS) that allows you to deploy AWS services directly to the edge of 5G networks.
Wavelength Zones are infrastructure deployments embedded within the data centers of telecommunications providers at the edge of 5G networks.
These zones are dedicated to specific 5G networks and are used to deploy AWS services directly on the edge.
By deploying services to a Wavelength Zone, you can ensure that users accessing your application through a mobile device on a 5G network experience extremely low latency.
With AWS Wavelength, you can deploy various AWS services to a Wavelength Zone, such as EC2 instances, EBS volumes, and VPCs.
For example, a telecom carrier with a 5G network can have a Wavelength Zone, and through a carrier gateway, you can deploy an EC2 instance on that zone.
The traffic within the Wavelength Zone typically stays within the communication service provider (CSP) network and does not reach AWS.
However, if you need to establish a secure connection to AWS, it is possible to connect the Wavelength Zone to the parent AWS region.
This allows EC2 instances in the Wavelength Zone to access AWS services like RDS or DynamoDB located in the main parent AWS region.

AWS Local Zones -
AWS Local Zones are an extension of AWS regions that allow you to place compute, storage, database, and other services closer to end users to run latency-sensitive applications. Local Zones are essentially additional availability zones (AZs) within a region that are located in different physical locations.
Key Points:
Local Zones are designed to reduce latency for applications that require low latency and high throughput.
Local Zones are compatible with various AWS services such as EC2, RDS, ECS, EBS, Elastic Cache, and Direct Connect.
By extending your AWS region to one or more Local Zones, you can distribute your resources across multiple locations, providing better performance for your users.
Local Zones are separate from the main AWS region but are still connected to it, allowing you to seamlessly manage resources across both.
Conclusion -
Local Zones in AWS provide low latency access to applications for users in specific geographic locations.
Enabling a Local Zone involves managing and updating the zone group.
Creating a subnet in a Local Zone allows you to deploy EC2 instances closer to your users.
This ensures faster access to your applications and improves the overall user experience.

Global Application Architecture -
1. Single Region, Single AZ
Consists of a single EC2 instance in one Availability Zone (AZ) within one region.
Does not provide high availability.
Does not improve global latency.
Simple to set up with low difficulty.

2. Single Region, Multi AZ
Consists of two AZs within one region.
Provides high availability.
Does not improve global latency significantly.
AZs are close to each other, so accessing from a distant location may result in high latency.
Slightly increased difficulty compared to single AZ setup.

3. Multi-Region, Active-Passive
Consists of two regions, each with one or multiple AZs.
One region is active, allowing users worldwide to perform reads and writes on the EC2 instance(s) in that region.
The other region is passive, with data replication from the active region.
Users can read from the passive region but cannot write to it.
Provides improved read latency globally due to data replication across regions.
Writes still have high latency as they need to go to the central active region.
Complexity increases with multiple passive regions.

4. Active-Active Architecture
In an active-active architecture, there are multiple regions or instances, and each region or instance is active and capable of handling reads and writes.
Replication still occurs within each active region or instance to ensure data consistency.
This architecture further improves read and write latency at a global level compared to active-passive architecture.
Setting up an active-active architecture is more complex as the application needs to handle multiple regions or instances simultaneously.

--> Cloud Integration
Cloud integration refers to the process of connecting multiple applications or services in a cloud environment to enable communication and data exchange between them. There are two main patterns for application communication: synchronous and asynchronous.
Synchronous Communication:
In synchronous communication, applications directly communicate with each other. This means that one application waits for a response from another application before proceeding. For example, in a buying and shipping service scenario, the buying service would directly communicate with the shipping service to initiate the shipping process.
Asynchronous/Event Based Communication:
In asynchronous communication, applications are decoupled and do not directly communicate with each other. Instead, they use a messaging system, such as a queue, to exchange messages. In this pattern, the buying service would put an order message in a queue, and the shipping service would read from the queue to process the orders.

Synchronous communication can be problematic if there are sudden spikes of traffic. In that case, it's better to decouple your applications using SQS: queue model, SNS: pub/sub model or Kinesis: real time big data streaming model and these services can scale independently from our application.

Amazon SQS -
Simple Queue Service is a service provided by AWS that allows for the decoupling of applications.
It enables producers to send messages into a queue, which can then be read and processed by consumers.
Producers and consumers can be single or multiple entities.
SQS allows for the sharing of work among consumers, with each consumer receiving different messages.
Once a consumer is done processing a message, it can delete the message from the queue.
Key Features:
SQS is fully managed and serverless, meaning there is no need to provision servers. SQS offers low latency.
It is designed to decouple applications, allowing them to operate at different speeds.
SQS can scale seamlessly from handling one message per second to tens of thousands of messages per second.
The default retention period for messages is four days, with a maximum of 14 days.
There is no limit to the number of messages that can be in a queue.
Messages must be processed within the retention period, as they will be automatically deleted.
Consumers share the work of processing messages, enabling parallel processing and increased efficiency.
How SQS Helps in Decoupling -
By inserting messages into an SQS queue instead of sending them directly to the video application, we can achieve decoupling between the web servers and the video processing layer. Here's how it works:
Web servers receive requests and send messages to an SQS queue.
The video processing layer, consisting of an auto scaling group with EC2 instances, reads messages from the SQS queue and processes the videos.

Amazon Kinesis -
Amazon Kinesis is a managed service that allows you to collect, process, and analyze real-time big data streaming data at any scale. It provides several sub-services to handle different aspects of data streaming and analysis:
1. Kinesis Data Streams
Kinesis Data Streams is a low-latency streaming service that allows you to ingest data at scale from hundreds of thousands of sources.
Sources can include various devices such as trucks, boats, IoT devices, or any other source that can produce data.
It is used to collect and store the streaming data for further processing and analysis.

2. Kinesis Data Firehose
Kinesis Data Firehose is a service that simplifies the process of loading data streams into different destinations.
Destinations can include Amazon S3, Amazon Redshift, Elasticsearch, and more.
It automatically scales to handle the incoming data and manages the delivery to the specified destinations.

3. Kinesis Data Analytics
Kinesis Data Analytics allows you to perform real-time analytics on the streaming data using SQL language.
It provides a simple and easy way to analyze the data and generate real-time insights.
You can use it to process and transform the data in real-time, and produce output based on the analysis.

4. Kinesis Video Streams
Kinesis Video Streams is a service specifically designed for monitoring real-time video streams.
It enables you to analyze video streams for various purposes, such as analytics or machine learning.

SNS -
SNS stands for Simple Notification Service. It is a service provided by AWS that allows you to send notifications to multiple subscribers. SNS uses a pub-sub (publish-subscribe) model, where an event publisher sends messages to an SNS topic, and the topic automatically sends notifications to all subscribed endpoints.
The event publishers only send message to one SNS topic.
Each SNS topic can have more than 12 million subscriptions/event subsribers per topic.
There is a soft limit of 100,000 topic limits for each AWS account.
SNS has various destinations it can publish messages to, including SQS, Lambda, Kinesis Data Firehose, email, SMS, mobile notifications, and HTTP/HTTPS endpoints.

Amazon MQ -
Amazon MQ is a managed message broker service provided by AWS. It allows you to use traditional open protocols, such as MQTT, AMQP, Stomp, Openwire, and WSS, when migrating your applications to the cloud. If you have a traditional application running on-premises that uses open protocols, you may not want to re-engineer your application to use AWS proprietary protocols like SQS and SNS. In such cases, Amazon MQ provides a solution by allowing you to use the protocols you are already familiar with.
Amazon MQ supports two technologies: RabbitMQ and ActiveMQ. These are popular on-premises message brokers that provide access to the open protocols mentioned earlier. With Amazon MQ, you can get managed versions of these brokers on the cloud.
Key Features and Considerations:
Amazon MQ provides both queue and topic features as part of a single broker. This means you can use it for both point-to-point messaging (like SQS) and publish-subscribe messaging (like SNS).
Unlike SQS and SNS, Amazon MQ does not scale as much.
Since Amazon MQ runs on servers, there is a possibility of server issues. To ensure high availability, you can set up a multi-AZ setup with failover.
Amazon MQ is primarily intended for companies migrating to the cloud and needing to use open protocols like MQTT, AMQP, Stomp, etc. If your application does not require these protocols, it is recommended to use SQS and SNS, as they scale better and are more tightly integrated with other AWS services.

--> Cloud Monitoring
Cloudwatch Metrics -
CloudWatch is a monitoring service provided by AWS that allows you to collect and track metrics, collect and monitor log files, and set alarms.
Cloudwatch allows you to create a dashboard to visualize all your metrics at once.
A metric is a variable that can be monitored, such as CPU utilization or network traffic.
Metrics have timestamps to track changes over time.
Metrics can be collected from various AWS services, such as EC2 instances, S3 buckets, RDS databases, and more.
Metrics can also be custom-defined, allowing you to monitor your own applications and services.
Some examples of metrics include the size and bytes of objects in an S3 bucket, the number of requests made to an S3 bucket, and the utilization of EC2 instances.

CloudWatch Alarms -
CloudWatch alarms are used to trigger notifications when a metric goes above or below a specified threshold.
Alarms can be set up to monitor any metric and take actions based on the metric's value.
Actions that can be triggered by alarms include:
Auto Scaling Group actions: Increase or decrease the number of EC2 instances in an Auto Scaling Group.
EC2 actions: Stop, terminate, reboot, or recover an EC2 instance.
SNS notifications: Send a notification to an SNS topic, such as an email or a text message.
Alarms can be created with various configurations, including:
Threshold: The value at which the alarm is triggered.
Evaluation period: The period over which the metric is evaluated, such as 5 minutes, 10 minutes, or an hour.
Actions: The actions to be taken when the alarm is triggered.
CloudWatch alarms have three possible states:
OK: Everything is within the defined threshold.
Insufficient Data: Not enough data points to determine the state.
Alarm: The metric has breached the defined threshold.

CloudWatch Logs -
CloudWatch Logs is a service provided by AWS that allows you to collect and monitor log files from various sources. Log files are generated by applications running on servers and contain information about the application's activities, such as user actions, system events, and error messages. CloudWatch Logs provides real-time monitoring of these logs, allowing you to react to events and troubleshoot issues effectively. CloudWatch Logs allows you to specify the retention period for your log data. This determines how long your logs will be stored and accessible within CloudWatch Logs.
Log Files and Collection -
Log files are text files that contain records of events and actions performed by an application.
They provide valuable information for troubleshooting and understanding the behavior of an application.
CloudWatch Logs can collect logs from various sources, including:
Elastic Beanstalk, ECS, Lambda, CloudTrail, CloudWatch Logs agent (installed on EC2 instances or on-premises servers), Route 53 (for logging DNS queries)
Example - CloudWatch Logs for EC2 Instances
By default, EC2 instances do not send log files to CloudWatch Logs. To enable log collection for EC2 instances, you need to follow these steps:
Install the CloudWatch Logs agent on your EC2 instances and it can also be set up on on-premises servers, making it a hybrid agent.
Configure the agent to send the desired log files to CloudWatch Logs.
Ensure that your EC2 instance has the necessary IAM role with the correct permissions to send log data to CloudWatch Logs.

Eventbridge -
Eventbridge is a service provided by AWS that allows you to react to events happening within your AWS account. It can be used for scheduling Cron jobs, monitoring events, and triggering actions based on specific conditions.
One of the use cases for Amazon Eventbridge is scheduling Cron jobs. You can create a rule in Eventbridge that specifies a recurring time interval, such as every 1 hour, and associate it with an event. This event can then trigger a script running on a Lambda function, effectively creating a serverless Cron job.
Another use case is reacting to specific events happening within AWS services. For example, you can create a rule in Eventbridge to monitor sign-in events for the IAM root user. Whenever someone signs in using the root user, an event will be generated and sent to an SNS topic, which can be combined with email notifications. This allows you to receive alerts whenever someone logs in using the root user.

Event Sources and Destinations:
EventBridge can receive events from both AWS services and external partners. Here are the different types of event sources:
Amazon Eventbridge supports a wide range of event sources, including EC2 instances, CodeBuild, S3, EventBridge itself, and many more. These event sources can generate events that are sent to Eventbridge for processing.
Eventbridge allows you to define various destinations for the events. Some examples of destinations include:
Lambda functions: You can trigger a Lambda function to execute a specific action in response to an event.
SNS topics: Events can be sent to an SNS topic, which can then be used to send notifications via email, SMS, or other supported protocols.
SQS queues: Events can be sent to an SQS queue for further processing or decoupling of components.

Event buses:
AWS Services: You can configure AWS services to send events to EventBridge. This includes services like AWS CloudTrail, AWS CloudWatch, AWS CodePipeline, and more. In Amazon Eventbridge, events are sent to event buses. The default event bus recieves events from various AWS services.
Partner Events: EventBridge also allows you to receive events from partner services that integrate with AWS. For example, if you use Zendesk or Datadog, they can send their own events into your AWS account through a partner event bus. This enables you to react to events happening outside of AWS.
Custom Applications: You can build your own custom applications and send events to EventBridge through a custom event bus. This gives you the flexibility to create custom integrations and tailor the event-driven architecture according to your specific needs.

Additional Capabilities:
In addition to receiving and processing events, EventBridge offers some additional capabilities:
Schema Registry: EventBridge provides a schema registry that allows you to model the event schema. This helps you understand the structure of the events, including the data types and other relevant information.
Event Archiving: You can choose to archive all the events sent to an event bus indefinitely or for a specific period. This allows you to store and analyze historical events for auditing, compliance, or other purposes.
Event Replay: EventBridge allows you to replay archived events. This means you can reprocess events that were previously sent to the event bus. This can be useful for testing, debugging, or recovering from failures.

CloudTrail -
CloudTrail is a service that provides governance, compliance, and audit capabilities for AWS accounts.
It enables the tracking and logging of all API calls and events that occur within an AWS account.
CloudTrail is enabled by default for all AWS accounts.
CloudTrail logs various types of activities, including: Console logins and actions, SDK usage, CLI commands, Service activity.
When creating a CloudTrail trail, you can choose to monitor activities in all AWS regions or limit it to a specific region.
This allows you to have a comprehensive view of activities across all regions(default) or focus on a specific region.
CloudTrail logs can be sent to two locations for storage:
CloudWatch Logs: CloudTrail logs can be sent to CloudWatch Logs for real-time monitoring and analysis.
Amazon S3: CloudTrail logs can be stored in an Amazon S3 bucket for long-term retention.

X-Ray -
X-Ray is a service provided by Amazon Web Services that allows for tracing and visual analysis of applications. It helps in troubleshooting performance issues, understanding dependencies in a microservice architecture, and identifying errors and exceptions in specific requests. X-Ray provides a service graph that helps you identify performance bottlenecks, latency issues, and errors in your application.

Challenges with Debugging in Production:
When applications are deployed in production, debugging can be challenging due to the following reasons:
Logs from different services and applications are scattered, making log analysis difficult.
Distributed services connected through various AWS services like SQS, SNS, and others make it hard to trace and understand the system's behavior.
Lack of a common view of the entire architecture makes it challenging to identify issues and bottlenecks.

Benefits of X-Ray:
Distributed Tracing: X-Ray allows you to trace requests as they move across different services and components in your application. This helps you understand the end-to-end flow and identify any issues or bottlenecks.
Troubleshooting: X-Ray provides detailed insights into the performance of your application, including latency, errors, and fault rates. This helps you troubleshoot and optimize your application for better performance.
Service Graph: X-Ray generates a service graph that visualizes the interactions between different components and services in your application. This helps you understand the dependencies and relationships between various parts of your application.

CodeGuru -
Amazon CodeGuru is a ML-powered service that offers automated code reviews and application performance recommendations. It aims to assist developers in improving the quality and performance of their code by providing actionable insights and suggestions.

Automated Code Reviews with CodeGuru Reviewer:
CodeGuru Reviewer performs automated code reviews using static code analysis.
It analyzes the lines of code in your repository (e.g., CodeCommit, BitBucket, GitHub) and provides recommendations based on its machine learning capabilities.
It can detect bugs, memory leaks, and other issues before they are identified by human reviewers.
It currently supports Java and Python, allowing you to review code written in these languages.
Benefits of CodeGuru Reviewer:
Identifies critical issues, security vulnerabilities, and hard-to-find bugs.
Helps implement coding best practices.
Finds resource leaks and other potential problems.

Application Performance Recommendations with CodeGuru Profiler:
CodeGuru Profiler offers visibility and recommendations for application performance during runtime, specifically in production environments.
It optimizes expensive lines of code before production by detecting and suggesting improvements during the build and testing phase.
During deployment, it measures the application in real-time and identifies performance and cost improvements.
CodeGuru Profiler supports applications running on AWS cloud or on-premises.
Recommendations from CodeGuru Profiler are provided directly in your code.
Benefits of CodeGuru Profiler:
Improves application performance and reduces costs.
Helps optimize resource usage.
Provides real-time insights into performance bottlenecks.

AWS Health Dashboard -
The AWS Health Dashboard is a tool provided by Amazon Web Services (AWS) that allows users to monitor the health and status of their AWS services and resources. It consists of two parts: the Service History and the Account Health Dashboard.

Service History:
The Service History section of the AWS Health Dashboard provides information about the health of AWS services in different regions.
Users can view the behavior and status of services on a daily basis, allowing them to track any issues or incidents that may have occurred.
The Service History also offers an RSS feed that users can subscribe to for regular updates on service health.

Account Health Dashboard:
The Account Health Dashboard focuses on the health of the user's AWS account and resources.
It is a global service that provides alerts, displays outages and remediation guidance when AWS experiences events that directly impact the user's account.
While the Service History displays the overall status of all services, the Account Health Dashboard specifically shows the performance and availability of services being used by the user or underlying your AWS resources.
The Account Health Dashboard offers relevant and timely information, including proactive notifications about scheduled maintenance activities.
Users can aggregate data for their entire AWS organization within the Account Health Dashboard.

--> VPC Networking
IP Addresses in AWS -
In AWS, there are two types of IP addresses: IPv4 and IPv6.
IPv4:
IPv4 stands for Internet Protocol version 4 and are 32-bit numbers.
IPv4 addresses have a capacity of about 4.3 billion addresses.
Public IPv4 addresses can be used on the Internet and are assigned to EC2 instances.
When you create an EC2 instance, it is assigned a new public IPv4 address.
If you stop and start the instance, it will be assigned a new public IPv4 address.
Private IPv4 addresses are internal IPs that can be used within your own network.
Private IPv4 addresses remain constant throughout the lifecycle of an EC2 instance, even if you stop and start it.

Elastic IPs:
Elastic IPs allow you to create a fixed IPv4 address and attach it to your EC2 instance.
If you stop and start the EC2 instance with an Elastic IP attached, it will retain the same public IPv4 address.
Elastic IPs can be beneficial if you need a fixed public IPv4 address.
However, there is a cost associated with Elastic IPs.
If the EC2 instance is stopped or the Elastic IP is not attached to anything, there will be an ongoing cost.
This is because AWS needs to pay to keep the public IP alive within its network.

IPv6:
IPv6 is the next evolution of the Internet Protocol. IPv6 addresses have a significantly larger capacity than IPv4 addresses.
Every IP address is public (no private range). Example - 2001:0db8:85a3:0000:0000:8a2e:0370:7334 (128-bit numbers)
IPv6 addresses are not as commonly used as IPv4 addresses in AWS, but they are available for use. You can enable your Virtual Private Cloud (VPC) to use IPv6 addresses as well.

VPC -
Virtual Private Cloud allows you to deploy your resources in a private network within AWS. It is linked to a specific region, so if you have multiple regions, you will need multiple VPCs.
Subnets:
A subnet is a range of IP addresses in your VPC. Subnets allow you to divide your VPC's IP address range into smaller, more manageable segments. 
It can be said subnet is a partition of your network and is associated with an availability zone (AZ). There are two types of subnets:
1. Public Subnet
This subnet is accessible from the internet.
To make a subnet public, you need to create an Internet Gateway and associate it with your VPC.
The public subnet must have a route to the Internet Gateway to enable internet access.
Instances launched in a public subnet can communicate with the internet and can be accessed from the internet.
2. Private Subnet
This subnet is not accessible from the internet. It provides a more secure environment for resources that do not require internet access, such as databases.
Instances launched in a private subnet cannot communicate with the internet directly.
However, you may still want instances in a private subnet to have internet access for tasks like downloading updates or files.
To provide internet access to instances in a private subnet, you can create a NAT Gateway or a NAT Instance.
A NAT Gateway is a managed service provided by AWS, while a NAT Instance is a self-managed instance.
The NAT Gateway or NAT Instance is created in the public subnet and acts as a bridge between the private subnet and the internet.
You need to create a route from the private subnet to the NAT Gateway/NAT Instance and from the NAT Gateway/NAT Instance to the Internet Gateway to enable internet connectivity for the private subnet.

Route Tables:
To define access to the internet and communication between subnets, you need to use route tables. Route tables determine how traffic is routed within the VPC.

VPC Structure:
A complete VPC structure within AWS consists of the following components:
AWS Cloud: The overall cloud environment provided by AWS.
Regions: Within the AWS cloud, there are multiple regions.
VPC: Each region can have one or more VPCs.
CIDR Range: Each VPC has a CIDR range, which is a range of IP addresses allowed within the VPC.
Availability Zones (AZs): Each VPC can span across two or more availability zones. Each availability zone can contain subnets, including public and private subnets.

Network ACL & Security Group -
Network ACLs:
Network Access Control Lists are another layer of security for your VPC.
They operate at the subnet level and control inbound and outbound traffic.
The default network ACL allows all traffic on all ports and protocols from anywhere.
You can create custom network ACLs and define specific rules to allow or deny traffic in and out of your subnets.
Each rule in a network ACL has a rule number and specifies the type of traffic to allow or deny.
Rules only include IP addresses.
Security Groups:
Security groups act as virtual firewalls that controls traffic to and from an ENI / an EC2 instance.
Security groups control traffic at the instance level.
They can have only allow rules.
Rules include IP addresses and other security groups.

VPC Flow Logs -
VPC flow logs are logs that capture all the IP traffic going through your interfaces within a Virtual Private Cloud (VPC). These logs provide valuable information for monitoring and troubleshooting connectivity issues within your VPC.
It can capture network information from AWS managed interfaces like ELB, ElastiCache, RDS, Aurora etc.
VPC Flow logs data can go to S3, CloudWatch Logs and Kinesis Date Firehose.

Types of Flow Logs:
VPC Flow Logs: Captures traffic going in and out of the entire VPC.
Subnet Flow Logs: Captures traffic going in and out of a specific subnet within the VPC.
Elastic Network Interface Flow Logs: Captures traffic going in and out of a specific elastic network interface.

VPC Peering -
VPC peering allows you to connect two VPCs privately using the AWS network, making them behave as if they were part of the same network.
The IP address ranges of the VPCs being peered should not overlap. If they do overlap, a VPC peering connection cannot be established.
VPC peering connections are not transitive. This means that if you have VPC A peered with VPC B, VPC A peered with VPC C and you want VPC B to communicate with VPC C, you need to create a separate peering connection between VPC B and VPC C.

Benefits of VPC Peering:
Enables communication between VPCs without the need for internet gateways, VPN connections, or NAT devices.
Allows resources in different VPCs to securely communicate with each other using private IP addresses.
Simplifies network architecture and reduces data transfer costs.

VPC Endpoints -
When connecting to AWS services, we typically connect to them over the public Internet. However, AWS provides a feature called VPC endpoints that allows us to connect to these services using a private AWS network instead. This offers several advantages, including improved security and reduced latency.

Types of VPC Endpoints:
There are two types of VPC endpoints that we can use to connect to AWS services:
VPC Endpoint Gateway: This type of endpoint is specifically for connecting to Amazon S3 and DynamoDB services. It allows us to connect to these services privately within our VPC private subnet. By using a gateway endpoint, we can avoid going over the public Internet and improve security.
VPC Endpoint Interface: This type of endpoint is used to connect to any other AWS service that is not Amazon S3 or DynamoDB. For example, if we want to push a custom metric from an EC2 instance inside of VPC private subnet to CloudWatch, we would create an interface endpoint for CloudWatch. This allows the EC2 instance to connect to CloudWatch privately.

PrivateLink -
PrivateLink is a feature within the VPC endpoint services family that allows you to connect a service running within your VPC to other VPCs directly and privately. It provides a secure and scalable way to establish connectivity between services without the need for VPC peering or an Internet gateway.

How it works:
Scenario: Let's say there is a vendor in AWS Marketplace and they run a service on their AWS account within their own VPC and they want to expose a service to customers of AWS. So, to thousand of VPC they need to have private access to establish a connectivity and we can go for VPC peering in this scenario but it doesn't scale and not very secure. We use PrivateLink which allows you to connect a service running within your VPC to other VPC directly and privately.
Vendor Setup: The vendor will create a network load balancer to expose their service within their own VPC.
Your Setup: You will create an elastic network interface (ENI) within your VPC.
Establishing Private Link: You will establish a private link between your ENI and the vendor's network load balancer. This creates a direct and private connection between the two VPCs.
Private Access: With the private link established, you now have private access to the vendor's service. All communication between your VPC and the vendor's VPC remains within the private network, ensuring security and privacy.
Scalability: If the vendor needs to provide access to additional customers, they can simply create new private links for each customer. This makes it easy to manage and highly scalable.

Site-to-Site VPN & Direct Connect -
Site-to-Site VPN:
A site-to-site VPN allows you to securely connect your on-premises network to your VPC over the public internet. It establishes a secure and encrypted connection between your on-premises network and your VPC.

To establish a site-to-site VPN, you need the following components:
Customer Gateway (CGW): This is a physical device or software application on your on-premises network that acts as the gateway for the VPN connection.
Virtual Private Gateway (VGW): This is a logical representation of an Amazon VPN gateway that you create in your VPC.
Site-to-Site VPN Connection: This is the connection between the CGW and VGW that allows traffic to flow securely between your on-premises network and your VPC.

Direct Connect (DX):
Direct connect is a physical connection between your on-premises data center and AWS. Here are some key points about direct connect:
It establishes a private, secure, and fast connection between your on-premises data center and the VPC.
It uses a private network instead of the public Internet, ensuring better security and reliability.
It is more expensive compared to site-to-site VPN because it requires a physical connection with a direct connect partner and takes at least a month to set up.

Client VPN -
The AWS Client VPN allows you to privately connect your computer to your AWS Virtual Private Cloud (VPC) or on-premises network using the OpenVPN protocol. This enables you to access resources in your VPC or on-premises network securely and easily.

Why use AWS Client VPN?
Accessing EC2 instances in a private VPC: If you have deployed EC2 instances in a private VPC, you can use AWS Client VPN to access them using their private IP addresses. This is particularly useful when you want to connect to your instances securely without exposing them to the public internet.
Establishing a site-to-site VPN connection: If your VPC has established a site-to-site VPN connection to your on-premises data center, AWS Client VPN allows your computer to access your on-premises servers privately. This means you can securely connect to your on-premises resources as if you were on the same network.

How it works:
Install the AWS Client VPN software on your computer.
Establish a VPN connection over the public internet to your VPC or on-premises network.
Once the VPN connection is established, you can access your resources in the VPC or on-premises network using their private IP addresses.
If your VPC has a site-to-site VPN connection to your on-premises data center, you can also access your on-premises servers privately.

Transit Gateway -
The Transit Gateway is a service provided by AWS that simplifies the network topology of a complex infrastructure. It allows for the connection of thousands of VPCs and on-premises systems through a hub and spoke star connection.
The Transit Gateway acts as a central hub, with all connections being made through it.
It eliminates the need for VPC peering, multiple connections, and complex routing between VPCs, VPN connections, and Direct Connect gateways.

--> Security & Compliance
Shared Responsibility Model -
AWS responsibility: Security of the cloud
Infrastructure (hardware, software, facilities, networking)
Managed services (S3, DynamoDB, RDS)

Customer responsibility: Security in the cloud
Management of operating systems (patching, updates)
Configuration of firewall (network ACL, security group)
Correct IAM permissions for EC2 instances
Encryption of application data according to compliance requirements

Shared Controls:
Patch management, Configuration management, Awareness and training

DDoS Attack Protection -
A Distributed Denial of Service (DDoS) attack is a malicious attempt to disrupt the normal functioning of a server or network by overwhelming it with a flood of internet traffic. To protect ourselves from such attacks, we can implement various measures on AWS.

Shield is a service provided by AWS that offers protection against Distributed Denial of Service (DDoS) attacks. It consists of two components: 
Shield Standard:
Shield Standard is a free service that is automatically activated for every AWS customer.
It provides protection against common DDoS attacks, such as SYN floods, UDP floods, reflection floods, and other layer 3 and layer 4 attacks.
Shield Advanced:
Shield Advanced is an optional service that costs $3,000 per month per organization.
It offers protection against more sophisticated attacks on EC2, ELB, CloudFront, Global Accelerator, and Route 53.
With Shield Advanced, you also gain access to a response team that can assist you during DDoS attacks.
Any fees incurred during an attack are covered by AWS.

Web Application Firewall (WAF) and Web ACL
The Web Application Firewall (WAF) is a service provided by AWS that helps protect your web applications from common web exploits. It specifically focuses on layer 7 attacks, which are attacks that target the HTTP protocol. WAF can be deployed on HTTP-friendly devices such as the Application Load Balancer and API Gateway. It can be used in combination with other services like AWS Shield, CloudFront, and Route 53 to provide comprehensive DDoS protection.
A Web ACL is a set of rules defined on a WAF to control access to web applications.
These rules can include filtering based on various criteria such as IP addresses, HTTP headers, request body, and specific strings.
By defining these rules, you can protect your web application against common attacks like SQL injection and cross-site scripting (XSS).
You can also set size constraints to ensure that requests are not too large and block access from specific countries using a geomatch.
Rate-based rules count the occurrences of events, such as the number of requests per second from a user. By setting a limit, such as five requests per second, you can prevent a single user from overwhelming your application with excessive requests.

CloudFront and Route 53
CloudFront is a content delivery network (CDN) service provided by AWS.
It caches your content at edge locations worldwide, reducing the load on your application server and improving performance.
When combined with AWS Shield, CloudFront provides DDoS attack mitigation at the edge locations, protecting your application from being overwhelmed.
Route 53, AWS's DNS service, can be used to route traffic to your application server.
By using Route 53 with AWS Shield, your DNS infrastructure is protected from DDoS attacks.

Penetration Testing -
Penetration testing is a security assessment technique where you intentionally try to attack your own infrastructure to identify vulnerabilities and test the effectiveness of your security measures.

Authorized Penetration Testing:
AWS allows customers to carry out security assessments and penetration testing on their own infrastructure without prior approval for the following eight services:
Amazon EC2 instances, NAT gateways, Load balancers, RDS, CloudFront, Aurora, API gateways, Lambda and Lambda Edge functions, Lightsail resources and Elastic Beanstalk environments. These services are explicitly authorized for penetration testing, meaning you can perform security assessments on them without seeking approval from AWS.

Prohibited Activities:
While the above-mentioned services are authorized for penetration testing, there are certain activities that are strictly prohibited. These activities include:
DNS Zone Walking, DDoS Attacks, Port Flooding, Protocol Flooding, Request Flooding

Encryption -
Encryption is a crucial aspect of data security in AWS. It involves converting data into a format that is unreadable and unintelligible to unauthorized individuals. AWS provides two types of encryption: encryption at rest and encryption in transit.

Encryption at Rest:
Encryption at rest refers to the encryption of data when it is stored or archived on a physical device, such as a hard drive or a database. It ensures that even if the physical device is compromised, the data remains protected. Some examples of encryption at rest in AWS include:
Encrypted data on an EFS (Elastic File System) network drive
Encrypted data on Amazon S3 (Simple Storage Service)

Encryption in Transit:
Encryption in transit, on the other hand, involves encrypting data while it is being transferred from one location to another. This ensures that the data remains secure during transit and cannot be intercepted or tampered with. Examples of encryption in transit in AWS include:
Transferring data from on-premises data centers to AWS
Moving data between an EC2 (Elastic Compute Cloud) instance and a DynamoDB table
Transferring data from EFS to Amazon S3

To ensure comprehensive data protection, it is recommended to encrypt data in both states: at rest and in transit. By encrypting data in both states, even if unauthorized individuals gain access to the data, they will not be able to decrypt and read it without the encryption keys.
They are used to encrypt and decrypt the data. Encryption keys are essential for maintaining the confidentiality and integrity of the data.

AWS Key Management Service (KMS)
KMS is the encryption service provided by AWS. It is used to manage encryption keys for various AWS services.
With KMS, AWS manages the keys for us, and we define who can access these keys.
Encryption with KMS is optional, and we can choose to enable it for different AWS services.
Examples of services that can be encrypted with KMS:
EBS volumes, S3 buckets, Redshift databases, RDS databases and EFS files
Some services, like CloudTrail logs, S3 Glacier, and Storage Gateway, have encryption enabled by default.

Cloud HSM (Hardware Security Module)
Cloud HSM is another encryption service provided by AWS.
With Cloud HSM, AWS provisions the encryption hardware, but we manage the keys ourselves.
The hardware used for encryption is called a Hardware Security Module (HSM).
AWS ensures that the HSM device is tamper-resistant and compliant with security standards.
Cloud HSM works by:
AWS manages the hardware (HSM device).
We use the Cloud HSM service and client to manage the keys.
The connection between our client and Cloud HSM is encrypted for secure key management.

Customer Master Keys (CMK)
AWS provides different types of Customer Master Keys (CMK) that can be used to encrypt and decrypt data. These keys are used to protect sensitive information and ensure the security of AWS resources. There are four types of CMKs in AWS:

Customer Managed CMK:
These keys are created, managed, and used by AWS users themselves.
Users have full control over these keys, including the ability to enable, disable, and define rotation policies.
Users can generate new keys periodically while preserving the old keys.
Users can also bring their own keys.

AWS Managed CMK:
These keys are created, managed, and used by AWS on behalf of the customer.
Users do not have direct access to these keys.
AWS services, such as AWS S3, AWS EBS, and Amazon Redshift, use these keys to encrypt and decrypt data.

AWS Owned CMK:
These keys are a collection of CMKs owned and managed by AWS.
They are used by AWS to protect resources in multiple customer accounts.
Users do not have access to view or manage these keys.

Cloud HSM Keys (Custom Key Store):
This type of key is generated directly from the user's own Cloud HSM hardware device.
Cryptographic operations using these keys do not happen within AWS Key Management Service (KMS), but within the Cloud HSM cluster.

AWS Certificate Manager (ACM) -
AWS Certificate Manager is a service provided by Amazon Web Services that allows you to easily provision, manage, and deploy SSL/TLS certificates. These certificates are used to provide in-flight encryption for your websites by enabling HTTPS endpoints.
SSL/TLS certificates are used to provide in-flight encryption for websites and enable HTTPS endpoints.
SSL/TLS certificates enable HTTPS endpoints, ensuring secure communication between clients and servers.
SSL/TLS certificates help protect sensitive information, such as login credentials and personal data, from being intercepted or tampered with during transmission.
ACM supports both public and private TLS certificates, with public certificates being free of charge.
ACM provides automatic TLS certificate renewal and integrates with various AWS services like ELB, CloudFront, APIs on API Gateway for seamless deployment of certificates.

Secrets Manager -
Secrets Manager is a service provided by AWS that allows you to securely store and manage secrets. It is designed specifically for storing sensitive information such as passwords, API keys, and database credentials.

Features of Secrets Manager:
Secret Rotation: Secrets Manager allows you to automate the rotation of secrets. This means that you can set a schedule for secrets to be automatically changed at regular intervals, such as every 90 days. This helps improve security by ensuring that secrets are regularly updated.
Integration with Amazon RDS: Secrets Manager has seamless integration with Amazon RDS (Relational Database Service). This means that you can use Secrets Manager to automatically generate and manage passwords for your RDS databases. The secrets are encrypted using AWS Key Management Service (KMS) for added security.
Encryption: Secrets stored in Secrets Manager are automatically encrypted using AWS KMS. This ensures that your secrets are protected and can only be accessed by authorized users or services.

AWS Artifacts -
AWS Artifacts is a portal provided by Amazon Web Services (AWS) that gives customers access to compliance reports and AWS agreements. It is not a service in itself, but it is presented as one in the AWS console. The purpose of AWS Artifacts is to provide customers with the necessary documentation to demonstrate AWS security and compliance.

Amazon GuardDuty -
Amazon GuardDuty is a service provided by AWS that helps protect your AWS account by performing intelligent threat discovery. It uses machine learning algorithms, performs automatic detection, and utilizes third-party data to identify potential threats. Enabling GuardDuty is as simple as a single click, and it offers a 30-day trial period without the need to install any additional software. It can also protect against CryptoCurrency attacks. We can set EventBridge rules to be notified in case of any findings. EventBridge rules can target AWS Lambda or SNS.

GuardDuty analyzes various input data sources to identify potential threats. These include:
CloudTrail Event Logs:
GuardDuty examines CloudTrail logs to detect unusual API calls and unauthorized deployments.
CloudTrail Management Events: GuardDuty monitors management events such as the creation of VPC subnets to identify any suspicious activities.
CloudTrail Data Events: GuardDuty analyzes data events, such as "get object," "list objects," and "delete objects" in S3, to detect any unauthorized access or data manipulation.

VPC Flow Logs:
GuardDuty examines VPC flow logs to identify unusual internet traffic, unusual IP addresses, and potential compromises.

DNS Logs:
GuardDuty analyzes DNS logs to detect EC2 instances sending encoded data within DNS queries, which could indicate a compromise.

Optional Features:
In addition to the mandatory input data sources, GuardDuty also offers optional features that allow you to analyze additional input data sources. These include EBS Volumes, Lambda, RDS and Aurora Login Activity, EKS Audit Logs.

Amazon Inspector -
Amazon Inspector is a service provided by AWS that allows you to run automated security assessments on various components of your infrastructure. It helps you identify vulnerabilities and potential security issues in your EC2 instances, container images on Amazon ECR, and Lambda functions.
It performs continuous scanning of the infrastructure to identify any vulnerabilities.
It relies on a database of vulnerabilities, such as CVE for package vulnerabilities, to compare against the components being assessed.
If the database of vulnerabilities is updated, Amazon Inspector automatically runs another assessment to ensure that all infrastructure is tested again.
Once Amazon Inspector completes its assessment, it can report its findings to the AWS Security Hub.
Additionally, Amazon Inspector can send findings and events to Amazon EventBridge.

AWS Config -
AWS Config is a service provided by Amazon Web Services (AWS) that helps with auditing and recording the compliance of your resources by tracking the configuration and changes over time. It allows you to have a comprehensive list of all the changes made to your AWS resources, providing visibility and control over your infrastructure.

Key Features:
Storage and Analysis: The configuration data recorded by AWS Config can be stored in Amazon S3, allowing you to analyze it later using services like Amazon Athena. This enables you to gain insights and perform audits on your resources.

Compliance Monitoring: AWS Config helps you address compliance-related questions by providing visibility into the configuration of your resources. For example, you can check if there is unrestricted SSH access to your security groups or if your S3 buckets have any public access.

Config Rules: Config Rules are customizable rules that you can define to evaluate the compliance of your resources. These rules can be used to check for specific configurations or enforce best practices. AWS Config can send alerts through SNS notifications whenever a resource violates a rule.

Multi-Region and Multi-Account Support: While AWS Config is a per-region service, you can create multiple configurations and aggregate the results across all your accounts and regions. This allows you to have a centralized view of your resources' configurations.

Amazon Macie -
Macie is a fully managed data security and data privacy service in AWS. Macie analyzes data stored in S3 buckets.
It utilizes machine learning and pattern matching to discover and protect sensitive data.
Specifically, it focuses on identifying personally identifiable information (PII).
It notifies users of these discoveries through EventBridge.
Users can integrate Macie with other AWS services like SNS topics and Lambda functions.

AWS Security Hub -
AWS Security Hub is a centralized security tool that allows you to manage security across multiple AWS accounts or within an organization. It automates security checks and provides an integrated dashboard to view the current security and compliance status. By aggregating alerts from various services and partner tools, such as GuardDuty, Inspector, Macie, IAM Access Analyzer, Systems Manager, Firewall Manager, and partner network solutions, Security Hub enables you to efficiently organize and respond to security alerts. Before using Security Hub, the AWS Config service needs to be enabled.

Key Features:
Centralized Security: Security Hub aggregates security findings from multiple accounts and services into one central location, allowing for better organization and streamlined actions.
Automated Checks: Security Hub automatically analyzes events from various services and partner tools, collecting potential security issues and findings.
Integrated Dashboard: The Security Hub dashboard provides a comprehensive view of security and compliance status, making it easier to identify and address security issues.
Event Generation: Security Hub generates events in the EventBridge service (CloudWatch Events) for further investigation and analysis.
Amazon Detective Integration: To investigate security events in more detail, Amazon Detective can be used to determine the root cause of incidents.

Amazon Detective -
Amazon Detective is a service provided by Amazon Web Services (AWS) that helps in analyzing and investigating security issues or suspicious activities within your AWS environment. It uses machine learning and graph theory to quickly identify the root cause of these issues.
Amazon Detective automatically collects and processes events from various sources within your AWS environment, including VPC flow logs, CloudTrail trails, and GuardDuty findings. It then creates a unified view of this data, allowing you to easily analyze and investigate security issues.

AWS Abuse -
If you suspect any abusive or prohibited behaviors on AWS, such as spam, illegal content, DDoS attacks, or malware distribution, you can report it to AWS. Here are some examples of abusive behaviors:
Spam: If you receive undesired emails from IP addresses associated with AWS websites and forums.
Spy and Bad EBIS Resources: Any resources used for spying or engaging in illegal activities.
Port Scanning: Someone conducting a port scan from an AWS instance, which is not allowed.
DDoS Attack Intrusion Attempts: Unauthorized attempts to log into your AWS resources.
Questionable or Copyrighted Content: Hosting illegal or copyrighted content on AWS.

To report any suspicious activities, you can contact the AWS abuse team. There are two ways to do this:
Online Form: Fill out the online form provided by AWS to report the abusive behavior.
Email: Send an email to abuse@amazonaws.com with the details of the suspected abuse.

Root User Privileges -
The root user in AWS is the account owner and is created when the account is first set up. The root user has complete access to all AWS services and resources. However, it is important to note that using the root user for everyday tasks is not recommended. Instead, it is advised to create a specific admin user for administrative tasks.

Here are some important actions that can only be performed by the root user:
Change Account Settings: Account name, Email address, Root user password, Root user access keys
View Certain Tax Invoices
Close Your Account
Restore IAM User Permissions
Change or Cancel AWS Support Plan
Register as a Seller in the Reserved Instance Marketplace:
Configure an Amazon S3 Bucket to Enable MFA
Edit or Delete an S3 Bucket Policy
Sign Up for GovCloud

IAM Access Analyzer -
IAM Access Analyzer is a service within the IAM console that helps identify which resources are being shared externally. This is important because sharing resources without proper authorization can pose a security risk to your company. IAM Access Analyzer specifically applies to the following resources:
S3 buckets
IAM roles
KMS keys
Lambda functions and layers
SQS queues
Secrets Manager secrets

The concept behind IAM Access Analyzer is to define a "zone of trust" that corresponds to your AWS accounts or your entire AWS organization. Anything outside of this zone of trust that has access to the resources mentioned above will be reported as findings. This allows you to take action if you believe there is a security risk.

--> Machine Learning
Amazon Rekognition -
It helps in finding objects, people, text, scenes in images and videos using ML. It offers facial analysis and facial search to do user verification, people counting. Use cases:
Labeling, Content Moderation, Text Detection, Face Detection and Analysis, Face Search and Verification, Celebrity Recognition and Pathing.

Amazon Transcribe -
Automatically convert speech to text. Uses a deep learning process called Automatic Speech Recognition (ASR) to convert speech to text quickly and accurately.
Automatically remove Personally Identifiable INformation (PII) using Redaction.
Supports Automatic Language Identification for multi-lingual audio.

Amazon Polly -
Amazon Polly is a service provided by Amazon Web Services (AWS) that allows you to convert text into lifelike speech using deep learning techniques. It enables you to create applications that can talk and generate audio output based on the provided text.

Amazon Translate -
Amazon Translate is a service provided by Amazon that offers natural and accurate language translation. It allows you to localize content, such as websites and applications, for international users. With Amazon Translate, you can easily translate large volumes of text efficiently.

Amazon Lex and Connect -
Amazon Lex:
Powers Alexa devices by Amazon
Converts speech into text using Automatic Speech Recognition (ASR)
Understands the intent of spoken words through Natural Language Understanding (NLU)
Used to build chatbots or call center bots
Amazon Connect:
Used to create virtual contact center
Receives calls and creates contact flows
Cloud-based and can integrate with other customer relationship management systems (CRMs) or AWS services
No upfront payment required. Approximately 80% cheaper than traditional contact center solutions

Amazon Comprehend -
Amazon Comprehend is a fully managed and serverless service provided by Amazon Web Services (AWS) that utilizes machine learning to analyze and understand text data. It is specifically designed for natural language processing (NLP) tasks.
Amazon Comprehend is used to extract insights and relationships from text data. It can understand the meaning of the text, extract key phrases, identify places, people, brands, and events mentioned in the text, perform sentiment analysis to determine the positivity or negativity of the text, automatically organizing a collection of text files by topic and analyze the text using tokenization and parts of speech.

Amazon Sagemaker -
Amazon Sagemaker is a fully managed service designed for developers and data scientists to build machine learning models. Unlike other managed machine learning services, Sagemaker provides a higher level of functionality and allows for more complex model creation. It simplifies the process of building machine learning models by providing tools and resources to assist at every step.

Amazon Forecast -
Amazon Forecast is a fully managed service that utilizes machine learning to provide highly accurate forecasts. It allows you to predict future outcomes based on historical time series data and additional product features. 50% more accurate than looking at the data itself. Reduce forecasting time from months to hours. Use cases: Product Demand Planning, Financial Planning, Resource Planning etc.

Amazon Kendra -
Amazon Kendra is a fully managed machine learning-powered document search service provided by AWS. It allows users to extract answers from various types of documents, including text, PDF, HTML, PowerPoint, Microsoft Word, and fax.

Features of Amazon Kendra:
Data Sources: Amazon Kendra supports a wide range of data sources where documents may be stored. These sources can include databases, file systems, websites, and more.

Knowledge Index: Amazon Kendra builds an internal knowledge index using machine learning techniques. This index is used to provide accurate and relevant search results.

Natural Language Search: Users can perform searches using natural language queries, similar to how they would search on platforms like Google. For example, a user can ask, "Where is the IT support desk?" and Kendra can provide the answer, "First floor," based on the information it has gathered from the indexed documents.

Incremental Learning: Amazon Kendra learns from user interactions and feedback to improve search results over time. This process is known as incremental learning, where the system adapts and promotes preferred search results based on user preferences.

Customization: Users can fine-tune search results based on various factors such as the importance of data, freshness, or custom filters. This allows for personalized and tailored search experiences.

Amazon Personalize -
Amazon Personalize is a fully managed machine learning service provided by Amazon that allows you to build applications with real-time personalized recommendations. It is the same technology used by Amazon.com to provide personalized product recommendations to its users based on their browsing and purchasing history.
Integrates into existing websites, applications, SMS, Email etc.

Amazon Textract -
Amazon Textract is a service provided by Amazon that allows you to extract text, handwriting, and data from scanned documents. It utilizes AI and machine learning algorithms to analyze the documents and provide the extracted information in a data file format.
Supports various document types such as driver licenses, PDFs, images, forms, and tables.

--> Account Management & Billing Support
Get an overview of all the videos if needed.

--> Advanced Identity
AWS STS -
The AWS Security Token Service (STS) is a service provided by AWS that allows you to create temporary limited privilege credentials to access your AWS resources. These temporary credentials are similar to your access key and secret access key, but they have an expiration period that you can configure.
STS works by assuming a role and we receive back temporary security credentials.
Use cases for STS include identity federation, IAM roles, and EC2 instances.

Amazon Cognito -
Amazon Cognito is a service provided by AWS that allows you to provide identity for your web and mobile application users. It is designed to handle potentially millions of users without the need to create IAM users for each individual user. Instead, you can create users for your mobile and web apps within Cognito.

Microsoft Active Directory -
Microsoft Active Directory (AD) is a database of objects found on any Windows server that has AD Domain Services. These objects can include user accounts, computers, printers, and file share security groups. AD provides centralized security management, allowing you to create accounts and assign permissions within it.
AWS Directory Services -
AWS Directory Services allows you to extend Active Directory functionality in the AWS cloud. While AWS itself does not have Active Directory, you can use AWS Directory Services to integrate with your on-premises Active Directory or create a new Active Directory in AWS.

There are three flavors of AWS Directory Services:
AWS Managed Microsoft AD: This allows you to create your own Active Directory in AWS and manage users locally. It also supports multi-factor authentication for enhanced security.

AD Connector: If you already have an on-premises Active Directory, you can establish a trust relationship between your on-premises AD and AWS. This allows users to access AWS resources using their existing AD credentials.

Simple AD: This is a standalone directory service compatible with Microsoft Active Directory. It provides basic AD functionality and is suitable for small-scale deployments.

AWS IAM Identity Center -
The AWS IAM Identity Center is a feature that provides single sign-on (SSO) capability for users and it is the successor to AWS Single Sign-ON. It allows users to have one login for all their AWS accounts within an organization. This feature is also applicable to business cloud applications, SAML 2.0 enabled applications, and EC2 Windows instances.
Benefits:
Single sign-on: Users only need to remember one login for all their AWS accounts.
Centralized user management: User management can be done in a central manner instead of on a per-account basis.
Simplified access: Users can access the management console of a specific account directly from the IAM Identity Center portal.