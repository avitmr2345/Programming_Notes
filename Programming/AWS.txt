--> Cloud Computing
Cloud computing is the on-demand delivery of compute power, database, storage, application, and other IT resources. It allows users to access these resources instantly, without the need for advance ordering or building their own data centers. Here are some key points about cloud computing:
On-demand delivery: Cloud computing provides resources when they are needed, allowing users to scale up or down as required.
Pay-as-you-go pricing: Users only pay for the resources they use, when they use them. There is no need for upfront investment or long-term commitments.
Provisioning: Cloud service platforms allow users to provision the exact type and size of computing resources they need. Whether it's a big server or a small one, the cloud can accommodate the user's requirements.
Instant access: Resources can be accessed instantly, without the need for 24 or 2 hours notice. Users can have access to servers, storage, databases, and application services within seconds.
Interface: Cloud computing provides a user-friendly interface that allows easy access to servers, storage, databases, and application services. This interface makes it convenient for users to manage and utilize their resources.

Types of Cloud Computing -
1. Infrastructure as a Service (IaaS)
Provides the fundamental building blocks for cloud computing.
Offers networking, computers, and data storage space in their raw form.
Provides a high level of flexibility and allows easy migration from traditional on-premises IT to the cloud.
Examples of IaaS services include Amazon EC2, Google Cloud, Azure, Rackspace, and Digitalocean.

2. Platform as a Service (PaaS)
Removes the need for organizations to manage the underlying infrastructure.
Focuses on the deployment and management of applications.
Allows developers to concentrate on developing and running their applications without worrying about infrastructure management.
AWS manages everything from the runtime to the networking.
Examples of PaaS services include AWS Elastic Beanstalk, Google App Engine, and Microsoft Azure App Service.

3. Software as a Service (SaaS)
Provides a complete product that is run and managed by the service provider.
Users can access the software through the internet without the need for installation or maintenance.
Eliminates the need for organizations to manage software updates, security, and infrastructure.
Examples of SaaS services include Salesforce, Google Workspace, Microsoft Office 365, and Dropbox.

Types of Clouds -
1. Private Cloud
Used by a single organization and not exposed to the public.
Examples: Rackspace
Provides a private data center managed by a third party.
Offers complete control and more security for sensitive applications.

2. Public Cloud
Owned and operated by a third-party cloud service provider.
Resources are delivered over the internet.
Examples: Microsoft Azure, Google Cloud, Amazon Web Services (AWS).
In this course, we will focus on AWS.

3. Hybrid Cloud
Hybrid cloud combines private and public cloud infrastructure.
Some servers are kept on-premises while certain capabilities are extended to the cloud.
Provides control over sensitive assets in the private infrastructure.
Offers flexibility and cost-effectiveness of the public cloud.

--> IAM(Identity & Access Management)
It is a Global service in which we are going to create our users and assign them to groups. Groups only contains users, not other groups. Users don't have to belong to a group, and user can belong to multiple groups. We create users or groups because we want to allow them to use our AWS accounts and we have to give them permissions. We add inline policies to those users who are not in any group. We protect users and groups by defining the password policy. The second option is using Multi Factor Authentication(MFA) which is password + security device you own to login the account. MFA devices options are - Virtual MFA device, Universal 2nd Factor Security Key etc.

Permissions -
Users or Groups can be assigned JSON documents called policies. These polices define the permissions of the users. In AWS you apply the least privilege principle that is don't give more permissions than a user needs.

IAM Policies Structure -
Version: policy language version
Id: an identifier for the policy (optional)
Statement: one or more individual statements (required)
Sid: an identifier for the statement (optional)
Effect: whether the statement allows or denies access
Principal: account/user/role to which this policy applied to
Action: list of actions or set of API calls this policy allows or denies based on the Effect
Resource: list of resources to which the actions applied to
Condition: conditions for when this policy is in effect (optional)

IAM Roles for Services -
Some AWS services will need to perform actions on your behalf. To do so, we'll assign permissions to AWS services with IAM Roles. Roles allows entities in the AWS to get credentials for a short duration and to do whatever they need to.

--> EC2
Elastic Compute Cloud is a service provided by AWS that allows users to rent virtual servers in the cloud, known as EC2 instances, and use them as compute resources. EC2 instances are bound to an Availibility Zones(AZ).
EC2 is composed of several components -
EC2 instances: Virtual machines that can be rented from AWS.
EBS volumes: Virtual drives used for storing data.
Elastic Load Balancer: Distributes load across multiple EC2 instances.
Auto Scaling Groups (ASG): Allows for scaling of services based on demand.

Choosing EC2 Instances -
When creating an EC2 instance, users have several options to choose from:
Operating System: Linux, Windows, or Mac OS.
Compute Power: Users can select the desired CPU and number of cores.
Random Access Memory (RAM): Users can choose the amount of RAM required.
Storage Space: Users can choose between network-attached storage (EBS or EFS) or hardware-attached storage (EC2 instance store).
Network Configuration: Users can select the type of network card and public IP.
Security Group: Users can define firewall rules for their EC2 instance.
Bootstrap Script (EC2 User Data): A script that configures the instance during its initial launch.

Instance Types -
Instances are categorized into families based on their intended use cases and performance characteristics.
Choosing the right instance type is important to ensure optimal performance for your application.
Some common instance families include:
General Purpose (e.g., t2, m5)
Compute Optimized (e.g., c5, r5)
Memory Optimized (e.g., x1, z1d)
Storage Optimized (e.g., i3, d2)
Naming convention:
m5.2xlarge (m is instance class, 5 is generation, 2xlarge is the size within the instance class)

EC2 User Data and Bootstrapping -
Bootstrapping refers to the process of launching commands or scripts when a machine starts.
The EC2 User Data script is run only once, during the initial boot of the instance.
The purpose of the EC2 User Data script is to automate boot tasks, hence the name bootstrapping.
The tasks you can automate using the User Data script can include installing updates, software, or downloading files from the internet.
The more tasks you add to your User Data script, the longer it will take for your instance to boot.
The EC2 User Data script runs with root user privileges, so any commands you include will have sudo rights.

Security Groups -
Security groups are a fundamental aspect of network security in the AWS cloud.
Security groups act as firewalls around EC2 instances that determine what traffic is allowed to enter and exit the EC2 instances.
Security groups can have rules that reference either IP addresses or other security groups.Rules define whether inbound traffic from the outside to the EC2 instance is allowed, and if the EC2 instance can perform outbound traffic to the internet.
Security groups can be attached to multiple EC2 instances. There is not a one-to-one relationship between security groups and instances.
Similarly, an EC2 instance can have multiple security groups attached to it.
Instances are associated with security groups, and security groups are associated with rules.

Impact of Region and VPC Changes -
Security groups are tied to a specific region and VPC combination.
If you switch to another region or create a new VPC, you need to create new security groups.
This means that security groups are not portable across regions or VPCs.
It is important to consider this when planning and managing your security groups.

Troubleshooting Security Group Issues -
If you are unable to connect to an EC2 instance on a specific port and your computer hangs without any response, it is likely a security group issue. This is because the security group is blocking the inbound traffic to that port. On the other hand, if you receive a "connection refused" error, it means that the security group allowed the traffic to reach the instance, but the application running on the instance is not responding or not launched properly.
By default, all inbound traffic is blocked and all outbound traffic is authorized.
This means that you can initiate outbound connections from your instances, but incoming connections are not allowed unless explicitly configured in the security group rules.

Referencing Security Groups from Other Security Groups -
AWS provides a feature that allows you to reference one security group from another security group. This is particularly useful when working with load balancers. By referencing a security group in the inbound rules of another security group, you can allow instances associated with the second security group to communicate with instances associated with the first security group. This eliminates the need to specify IP addresses and simplifies the configuration process.
For example, if you have an EC2 instance with security group "Group 1" and you authorized inbound traffic from "Group 2" then any instance associated with "Group 2" can communicate with the instance associated with "Group 1" on the specified port.

Importance of SSH Access -
SSH access is often the most critical and sensitive aspect of security. 
It is recommended to maintain a separate security group specifically for SSH access.
This helps ensure that SSH access is properly configured and secured.
By dedicating a separate security group, it becomes easier to manage and monitor SSH traffic.
SSH(Secure Shell) allows you to control a remote machine or server using CLI.
Linux and windows command - ssh -i '.\EC2 Tutorial.pem' ec2-user@65.2.57.62 (.pem is ssh key, 65.... is public IP address)

--> EC2 Instance Storage
EBS volumes -
Elastic Block Store (EBS) is a block-level storage service provided by AWS. It allows you to create and attach persistent block storage volumes to your EC2 instances. They allow data to persist even after the instance is terminated.
Each EBS volume can only be mounted to one instance at a time.
When creating EBS volumes, you need to specify the AZ in which the volume will be created & it is bound to that specific availability zone.
EBS volumes are network drives, not physical drives and are attached to instances through the network..
Communication between the instance and the EBS volume is done through the network, which may introduce some latency.
EBS volumes can be quickly detached from one EC2 instance and attached to another.
EBS volumes are locked to specific availability zones, meaning they can only be attached to instances in the same availability zone.
When creating EBS volumes through EC2 instances, there is an attribute called "delete on termination". This attribute determines whether the EBS volume should be deleted automatically when the associated EC2 instance is terminated.
By default:
The "Delete on Termination" option is ticked for the root volume.
The "Delete on Termination" option is not ticked for any other attached EBS volumes.

EBS Snapshots -
It makes a backup of your EBS volume at a point of time. It is not necessary to detach volume to do snapshot. Can copy snapshots across AZ or regions, so that you would be able to transfer some of your data in a different region.

AMI -
AMI stands for Amazon Machine Image. AMIs represent a customization of an EC2 instance.
AMIs can be created by AWS or customized by users. AMIs contain software configurations, including the operating system and monitoring tools.
Behind the scenes, when we create an AMI, EBS snapshots are also created.
These snapshots capture the data stored on the instance's EBS volumes.
They provide a backup of the instance's data and can be used to restore or create new volumes.
Benefits of Using AMI:
Faster boot time and configuration time.
Prepackaged software installation on EC2 instances.
Can be built for a specific region and copied across regions. Allows for leveraging the AWS global infrastructure.

Types of AMIs -
1. Public AMIs
Provided by AWS. Can be used by all AWS users. No need to create or maintain them.
Examples: Amazon Linux 2 AMI.

2. Custom AMIs
Created and maintained by users. Can be built for specific requirements. Automation tools available for AMI creation. Users are responsible for creating and maintaining them.

3. AWS Marketplace AMIs
Created by vendors and potentially sold by them. Can save time by using pre-configured software. Users can also create and sell their own AMIs on the AWS Marketplace.

AMI Creation Process -
AMIs are a powerful tool for creating customized EC2 instances with pre-configured software and settings.
Launching an Instance in us-east-1a:
We will start by launching an instance in the us-east-1a region.
This instance will serve as the basis for our custom AMI.

Creating a Custom AMI:
Once we have customized the instance, we will create an AMI from it.
This AMI will capture the configuration and state of the instance at the time of creation.
It will serve as a template for launching new instances with the same configuration.

Launching Instances from the Custom AMI in us-east-1b:
In the us-east-1b region, we will be able to launch instances from the custom AMI we created.
This will effectively create a copy of our EC2 instance in a different region. We can launch multiple instances from the same AMI if needed.

EC2 Image Builder -
EC2 Image Builder is a service provided by AWS that allows you to automate the creation, maintenance, validation, and testing of Amazon Machine Images (AMIs) for EC2 instances. This service simplifies the process of building and customizing software components on EC2 instances, ensuring that your AMIs are up-to-date and meet your specific requirements.

EC2 Image Builder Working -
Setup: You configure and set up the EC2 Image Builder service, specifying the desired components and customizations for your AMIs.
Builder EC2 Instance: When the EC2 Image Builder service runs, it automatically creates a builder EC2 instance. This instance is responsible for building components and customizing the software according to your specifications. For example, it can install Java, update the CLI, update the software system, install firewalls, or even install your application.
AMI Creation: Once the builder EC2 instance completes its tasks, EC2 Image Builder creates an AMI from that instance. This AMI represents the desired state of your EC2 instances, including all the customizations and software installations.
Validation: To ensure the quality and functionality of the AMI, EC2 Image Builder automatically creates a test EC2 instance from the newly created AMI. You can define a set of tests to be run on this instance, such as checking if the AMI is working correctly, if it is secure, or if your application is running properly. You have the flexibility to skip certain tests if needed.
Distribution: Once the AMI passes the validation tests, it is ready for distribution. Although EC2 Image Builder is a regional service, you can distribute the AMI to multiple regions, allowing your application and workflow to be globally available.

EC2 Instance Store -
The EC2 Instance Store is a type of hardware disk that can be attached directly to an EC2 instance. It provides extremely high disk performance and is a great choice when you need better I/O performance and good throughput.
The storage provided by the EC2 Instance Store is considered ephemeral, meaning that if you stop or terminate the EC2 instance, the data stored on the instance store will be lost. Therefore, it is not suitable for long-term storage.
The EC2 Instance Store is ideal for use cases where you need temporary storage or scratch data. It can be used as a buffer or cache for storing data that is not critical or does not need to be persisted long-term.

EFS -
EFS stands for Elastic File System and is a managed network file system.
It allows you to attach a network file system to multiple EC2 instances simultaneously.
Unlike EBS volumes, which can only be attached to one instance at a time, EFS can be mounted onto hundreds of instances.
EFS is a shared network file system, also known as a shared NFS (Network File System).
It is designed to work with Linux EC2 instances and this architecture allows for easy sharing and access to files across instances in different availability zones.

--> Elastic Load Balancing & ASG
Scalability and High Availability -
Scalability allows an application to handle greater loads by adapting.
There are two types of scalability in the cloud:
1. Vertical scalability
Increasing the size or capacity of an instance. Example: Changing from t2.micro to t2.large.
Vertical scalability is common in non-distributed systems like databases.

2. Horizontal scalability (elasticity)
Increasing the number of instances. Improves performance and capacity. Increase fault tolerance and availability.
In AWS, achieving horizontal scalability:
Using Elastic Load Balancing (ELB) to distribute traffic across multiple instances.
Auto Scaling Groups (ASG) to automatically add or remove instances based on demand.
Horizontal scalability is commonly used in distributed systems to handle large workloads.

Scalability is linked but different from high availability.
High availability refers to the ability of an application to remain operational and accessible even in the face of failures or disruptions. It is recommended to use at least two availability zones for high availability.
Achieving high availability in AWS:
Using Elastic Load Balancing (ELB) to distribute traffic across multiple instances.
Auto Scaling Groups (ASG) to automatically replace failed instances.
Deploying resources across multiple Availability Zones (AZs) to ensure redundancy.

Scalability vs Elasticity vs Agility
Scalability: Ability for a system to accommodate a larger load by making the hardware stronger (scaling up) or adding nodes (scaling out).
Elasticity: System's ability to automatically scale based on the load it receives.
Agility: New IT resources are only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes.

Elastic Load Balancing -
Elastic Load Balancing (ELB) is a service provided by AWS that allows for better scalability and availability of backend EC2 instances and they can be spread out across multiple AZ. It acts as a server that forwards internet traffic to multiple downstream servers, which are the backend EC2 instances. Load balancers handle upgrades, maintenance, and high availability, while users only need to configure their behavior.
Types of Load Balancers:
1. Application Load Balancer (ALB)
Layer 7 load balancer for HTTP and HTTPS traffic.
Supports HTTP routing features and static DNS for a static URL.
Simple architecture: Users access the load balancer using HTTP or HTTPS protocols.

2. Network Load Balancer (NLB)
Ultra-high performance load balancer.
Layer 4 load balancer for TCP and UDP traffic.
Suitable for applications that require low latency and high throughput.
Simple architecture: Users access the load balancer using TCP or UDP protocols.

3. Gateway Load Balancer (GWLB)
Layer 3 load balancer.
It is designed to route traffic to third-party security virtual appliances, such as firewalls, intrusion detection systems, or deep packet inspection systems.
GWLB allows you to inspect IP packets and perform firewall operations before forwarding the traffic to your applications.
It supports the Geneve protocol for routing traffic to virtual appliances running on EC2 instances.
GWLB architecture involves sending traffic to EC2 instances running virtual appliances, analyzing the traffic, and then forwarding it back to the load balancer for further routing to the applications.

Auto Scaling Groups -
An autoscaling group (ASG) is a feature provided by cloud service providers, that allows for the automatic creation and termination of instances based on the current demand or load on an application. ASGs are commonly used in conjunction with load balancers to ensure efficient and scalable application deployments. We can spread our load across multiple AZ.
The goal of ASG is to:
Scale out (add EC2 instances) to match an increased load.
Scale in (remove EC2 instances) to match a decreased load.
Ensure we have a min and max number of machines running.
Replace unhealthy instances.

ASG in AWS with Load Balancer -
When using an autoscaling group with EC2 instances, the load balancer plays a crucial role in distributing traffic.
Initially, if there is only one EC2 instance in the autoscaling group, the load balancer will redirect all incoming web traffic to that instance.
As the autoscaling group scales out by adding more EC2 instances, the load balancer will register them and distribute traffic across all instances.
The load balancer dynamically adjusts the distribution of traffic based on the number of instances available.
This ensures that the workload is evenly distributed and can handle increased traffic.
The load balancer can distribute traffic to the maximum size of the autoscaling group if it scales to that point.

--> S3
Amazon S3 stands for Simple Storage Service and is a cloud storage service provided by Amazon Web Services (AWS).
It allows you to store and retrieve large amounts of data, such as files, documents, images, videos, etc. It offers a wide range of use cases, including backup and storage, disaster recovery, archival purposes, hybrid cloud storage, hosting applications and media, data lakes, big data analytics, delivering software updates, and hosting static websites.
Files in Amazon S3 are called objects.
Files in Amazon S3 are stored in buckets, which can be seen as top-level directories.
Buckets are created in your account and must have a globally unique name.
S3 looks like a global service but buckets are created in a region.
S3 pre-signed URL contains a signature that verifies the credentials in the encoded URL to allow the user to show its object. 
S3 Objects:
Objects in S3 are files.
Each object has a key, which is the full path of the file.
The key of a file in the top-level directory is the same as the object name.
If a file is nested in folders, the key is the full path of the file. The key is composed of a prefix (folder path) and an object name.
S3 does not have a concept of directories, everything is a key.
Objects in Amazon S3 can have metadata associated with them. Metadata consists of a list of key-value pairs that provide additional information about the object. Metadata can be set by the system or by the user.
Each object can have Tags (up to 10 Unicode key-value pairs as metadata which are useful for security/lifecycle).
Object can have Version ID (if versioning is enabled).
Object Values:
The values of S3 objects are the content of the file.
You can upload any type of file to S3. There is a maximum size limit for objects in S3.

S3 Security -
1. User-Based Security
IAM policies determine which API calls are allowed for a user. IAM principles can access S3 objects if:
IAM permissions allow it.
Resource policies allow it.
There is no explicit deny in the action.

2. Resource-Based Security
(i) Bucket Policies
S3 bucket policies are bucket-wide rules that can be assigned from the S3 console.
Bucket policies allow specific users or users from other accounts (cross-account) to access the S3 bucket.
Bucket policies can be used to make S3 buckets public.
(ii) Object Access Control List (ACL)
ACL provides finer-grained security for S3 objects. ACL can be disabled.
(iii) Bucket-level ACL is less common but can also be disabled.

3. Object Encryption
Objects in S3 can be encrypted using encryption keys.

Bucket Policies -
Bucket policies are the most common way to secure an S3 bucket. Bucket policies are JSON-based policies.
The structure of a bucket policy:
Resource: Specifies the buckets and objects the policy applies to.

Granting Public Access
To grant public access to a bucket or objects within it, we can use an S3 bucket policy.
For example, we can set a bucket policy that allows anyone (*) to perform the GetObject action on all objects within the bucket.
This would make all objects within the bucket publicly readable.

Granting Access to IAM Users
If we have an IAM user within our AWS account who needs access to the S3 bucket, we can assign IAM permissions to that user through a policy.
By creating a policy that allows access to the S3 bucket, the user will be able to access the bucket.

Granting Access from EC2 Instances
If we want to give access from an EC2 instance to an S3 bucket, IAM users are not appropriate. Instead, we need to use IAM roles.
We can create an IAM role for the EC2 instance with the necessary IAM permissions to access the S3 bucket.
By assigning this role to the EC2 instance, it will be able to access the S3 bucket.

Allowing Cross-Account Access
If we want to allow access from an IAM user in another AWS account, we can use a bucket policy.
We can create an S3 bucket policy that allows cross-account access.
By specifying the IAM user in the other AWS account in the bucket policy, we can grant them access to the S3 bucket.

Static Website Hosting, Versioning, Replication, S3 Storage Classes -
Easy topics, if you forgot these watch a quick tutorial.

AWS Snow Family -
The AWS Snow Family is a collection of highly secure portable devices that serve two main use cases within AWS: data migration and edge computing. AWS OpsHub is a software to connect & manage your Snow Family Devices.
Data migration refers to the process of transferring large amounts of data in and out of AWS. The AWS Snow Family offers three different types of devices for data migration: Snow Cone, Snowball Edge, and Snowmobile and each option has different storage capacities 8 terabytes, 80 terabytes, and 100 petabytes respectively.
Recommended migration size for Snow Cone is up to 24 terabytes online and offline and DataSync agent is pre-installed on this.
Snowball Edge is suitable for petabyte migrations and requires offline data transfer & supports storage clustering to put 15 of them together.
Snowmobile is the best choice for transferring more than ten petabytes of data.

Edge computing is a concept where data processing and analysis is performed closer to the source of data creation, rather than sending it back to the cloud. AWS Snow Family provides a range of devices that enable edge computing capabilities which are Snowcone, Snowcone SSD, Snowball Edge and they can run EC2 instances & AWS Lambda functions on them.